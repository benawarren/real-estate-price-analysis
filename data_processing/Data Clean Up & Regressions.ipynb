{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "789a7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9831fed6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_c/t_p4w1f91_11yj717hx6qttc0000gn/T/ipykernel_98628/685495345.py:2: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  filtered = pd.read_csv('/Users/junsik/Documents/Dickinson College/Spring 2025/DATA 400/Final Project/Data/Final Project Data Jan 4 2025/filtered.csv')\n"
     ]
    }
   ],
   "source": [
    "# importing filtered data \n",
    "filtered = pd.read_csv('/Users/junsik/Documents/Dickinson College/Spring 2025/DATA 400/Final Project/Data/Final Project Data Jan 4 2025/filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "624ff560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>city</th>\n",
       "      <th>address</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>parcel_id</th>\n",
       "      <th>location</th>\n",
       "      <th>sale_date</th>\n",
       "      <th>sale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2625090</td>\n",
       "      <td>53162</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>1841 OAKMONT ST</td>\n",
       "      <td>19111.0</td>\n",
       "      <td>50016134.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-05 05:00:00+00:00</td>\n",
       "      <td>88000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2626395</td>\n",
       "      <td>54467</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>4516 ALDINE ST</td>\n",
       "      <td>19136.0</td>\n",
       "      <td>50015541.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-04 05:00:00+00:00</td>\n",
       "      <td>39515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2626751</td>\n",
       "      <td>54823</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>821 CATHARINE ST</td>\n",
       "      <td>19147.0</td>\n",
       "      <td>50018248.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-07 05:00:00+00:00</td>\n",
       "      <td>150000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2626765</td>\n",
       "      <td>54837</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>3311 W PENN ST</td>\n",
       "      <td>19129.0</td>\n",
       "      <td>50018066.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-07 05:00:00+00:00</td>\n",
       "      <td>115000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2626881</td>\n",
       "      <td>54953</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>211 N CECIL ST</td>\n",
       "      <td>19139.0</td>\n",
       "      <td>50018288.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-07 05:00:00+00:00</td>\n",
       "      <td>25000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794721</th>\n",
       "      <td>6322840</td>\n",
       "      <td>1097624</td>\n",
       "      <td>Stamford</td>\n",
       "      <td>193 SADDLE HILL ROAD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2200073</td>\n",
       "      <td>POINT (-73.577612999 41.148977983)</td>\n",
       "      <td>2022-10-11</td>\n",
       "      <td>865000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794722</th>\n",
       "      <td>6322841</td>\n",
       "      <td>1097625</td>\n",
       "      <td>Wethersfield</td>\n",
       "      <td>37 LUCA LN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>220369</td>\n",
       "      <td>POINT (-72.663607 41.712487)</td>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>760857.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794723</th>\n",
       "      <td>6322842</td>\n",
       "      <td>1097626</td>\n",
       "      <td>Stamford</td>\n",
       "      <td>1096 EAST MAIN STREET #16-D-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2200470</td>\n",
       "      <td>POINT (-73.515726977 41.057837988)</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>220000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794724</th>\n",
       "      <td>6322843</td>\n",
       "      <td>1097627</td>\n",
       "      <td>Old Saybrook</td>\n",
       "      <td>115 SHEFFIELD ST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22396</td>\n",
       "      <td>POINT (-72.368005967 41.289124997)</td>\n",
       "      <td>2023-09-26</td>\n",
       "      <td>1575000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794725</th>\n",
       "      <td>6322844</td>\n",
       "      <td>1097628</td>\n",
       "      <td>Southington</td>\n",
       "      <td>32 SOUTHINGTON AV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>220890</td>\n",
       "      <td>POINT (-72.87929502 41.590926988)</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>330000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1794726 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1  Unnamed: 0          city  \\\n",
       "0             2625090       53162  PHILADELPHIA   \n",
       "1             2626395       54467  PHILADELPHIA   \n",
       "2             2626751       54823  PHILADELPHIA   \n",
       "3             2626765       54837  PHILADELPHIA   \n",
       "4             2626881       54953  PHILADELPHIA   \n",
       "...               ...         ...           ...   \n",
       "1794721       6322840     1097624      Stamford   \n",
       "1794722       6322841     1097625  Wethersfield   \n",
       "1794723       6322842     1097626      Stamford   \n",
       "1794724       6322843     1097627  Old Saybrook   \n",
       "1794725       6322844     1097628   Southington   \n",
       "\n",
       "                               address  zip_code   parcel_id  \\\n",
       "0                      1841 OAKMONT ST   19111.0  50016134.0   \n",
       "1                       4516 ALDINE ST   19136.0  50015541.0   \n",
       "2                     821 CATHARINE ST   19147.0  50018248.0   \n",
       "3                       3311 W PENN ST   19129.0  50018066.0   \n",
       "4                       211 N CECIL ST   19139.0  50018288.0   \n",
       "...                                ...       ...         ...   \n",
       "1794721           193 SADDLE HILL ROAD       NaN     2200073   \n",
       "1794722                     37 LUCA LN       NaN      220369   \n",
       "1794723  1096 EAST MAIN STREET #16-D-1       NaN     2200470   \n",
       "1794724               115 SHEFFIELD ST       NaN       22396   \n",
       "1794725              32 SOUTHINGTON AV       NaN      220890   \n",
       "\n",
       "                                   location                  sale_date  \\\n",
       "0                                       NaN  2000-01-05 05:00:00+00:00   \n",
       "1                                       NaN  2000-01-04 05:00:00+00:00   \n",
       "2                                       NaN  2000-01-07 05:00:00+00:00   \n",
       "3                                       NaN  2000-01-07 05:00:00+00:00   \n",
       "4                                       NaN  2000-01-07 05:00:00+00:00   \n",
       "...                                     ...                        ...   \n",
       "1794721  POINT (-73.577612999 41.148977983)                 2022-10-11   \n",
       "1794722        POINT (-72.663607 41.712487)                 2023-09-29   \n",
       "1794723  POINT (-73.515726977 41.057837988)                 2023-01-09   \n",
       "1794724  POINT (-72.368005967 41.289124997)                 2023-09-26   \n",
       "1794725   POINT (-72.87929502 41.590926988)                 2023-05-18   \n",
       "\n",
       "         sale_price  \n",
       "0           88000.0  \n",
       "1           39515.0  \n",
       "2          150000.0  \n",
       "3          115000.0  \n",
       "4           25000.0  \n",
       "...             ...  \n",
       "1794721    865000.0  \n",
       "1794722    760857.0  \n",
       "1794723    220000.0  \n",
       "1794724   1575000.0  \n",
       "1794725    330000.0  \n",
       "\n",
       "[1794726 rows x 9 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c855390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: detroit_income\n",
      "DataFrame: detroit_race\n",
      "DataFrame: detroit_employment\n",
      "DataFrame: detroit_ed_attainment\n",
      "DataFrame: ct_race\n",
      "DataFrame: ct_income\n",
      "DataFrame: ct_employment\n",
      "DataFrame: ct_ed_attainment\n",
      "DataFrame: philly_race\n",
      "DataFrame: philly_employment\n",
      "DataFrame: philly_ed_attainment\n",
      "DataFrame: philly_income\n",
      "DataFrame: pittsburgh_race\n",
      "DataFrame: pittsburgh_employment\n",
      "DataFrame: pittsburgh_income\n",
      "DataFrame: pittsburgh_ed_attainment\n"
     ]
    }
   ],
   "source": [
    "# importing all ACS data \n",
    "# Define the path to the parent folder containing all subfolders\n",
    "parent_folder = '/Users/junsik/Documents/Dickinson College/Spring 2025/DATA 400/Final Project/Data/Final Project Data Jan 4 2025/ACS_data/merged_files'  # Replace with the actual path\n",
    "\n",
    "# Create an empty dictionary to store DataFrames\n",
    "acs_dataframes = {}\n",
    "\n",
    "# Loop through all subfolders and files in the parent folder\n",
    "for root, dirs, files in os.walk(parent_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):  # Check if the file is a CSV\n",
    "            file_path = os.path.join(root, file)\n",
    "            # Use the CSV file name (without extension) as the DataFrame name\n",
    "            df_name = os.path.splitext(file)[0]\n",
    "            acs_dataframes[df_name] = pd.read_csv(file_path)\n",
    "\n",
    "# Access individual DataFrames by their names\n",
    "for name, df in acs_dataframes.items():\n",
    "    print(f\"DataFrame: {name}\")\n",
    "#     print(df.head())  # Display the first few rows of each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "55b2efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importing all commerce data \n",
    "\n",
    "# # Define the path to the folder containing all CSV files\n",
    "# folder_path = '/Users/junsik/Documents/Dickinson College/Spring 2025/DATA 400/Final Project/Data/Final Project Data Jan 4 2025/commerce_data'  # Replace with the actual path to your folder\n",
    "\n",
    "# # Create an empty dictionary to store DataFrames\n",
    "# commerce_dataframes = {}\n",
    "\n",
    "# # Loop through all files in the folder\n",
    "# for file in os.listdir(folder_path):\n",
    "#     if file.endswith('.csv'):  # Check if the file is a CSV\n",
    "#         file_path = os.path.join(folder_path, file)\n",
    "#         # Use the CSV file name (without extension) as the DataFrame name\n",
    "#         df_name = os.path.splitext(file)[0]\n",
    "#         commerce_dataframes[df_name] = pd.read_csv(file_path)\n",
    "\n",
    "# # Access individual DataFrames by their names\n",
    "# for name, df in commerce_dataframes.items():\n",
    "#     print(f\"DataFrame: {name}\")\n",
    "# #     print(df.head())  # Display the first few rows of each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bf09e8cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # importing crime data \n",
    "\n",
    "# # Define the path to the folder containing all CSV files\n",
    "# folder_path = '/Users/junsik/Documents/Dickinson College/Spring 2025/DATA 400/Final Project/Data/Final Project Data Jan 4 2025/crime_data'\n",
    "\n",
    "# # Create an empty dictionary to store DataFrames\n",
    "# crime_dataframes = {}\n",
    "\n",
    "# # Loop through all files in the folder\n",
    "# for file in os.listdir(folder_path):\n",
    "#     if file.endswith('.csv'):  # Check if the file is a CSV\n",
    "#         file_path = os.path.join(folder_path, file)\n",
    "#         df_name = os.path.splitext(file)[0]  # Use the CSV file name (without extension) as the DataFrame name\n",
    "#         try:\n",
    "#             # Read the CSV file and skip bad lines\n",
    "#             crime_dataframes[df_name] = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# # Access individual DataFrames by their names\n",
    "# for name, df in crime_dataframes.items():\n",
    "#     print(f\"DataFrame: {name}\")\n",
    "# #     print(df.head())  # Display the first few rows of each DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a95cbd",
   "metadata": {},
   "source": [
    "## Cleaning up income ACS dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c96bc290",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Census Tract</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>Median Income (Households)</th>\n",
       "      <th>Median Income (Families)</th>\n",
       "      <th>Median Income (Married Families)</th>\n",
       "      <th>Median Income (Nonfamily Households)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Census Tract 5001</td>\n",
       "      <td>26163500100</td>\n",
       "      <td>36880.0</td>\n",
       "      <td>34637.0</td>\n",
       "      <td>65424.0</td>\n",
       "      <td>34859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Census Tract 5002</td>\n",
       "      <td>26163500200</td>\n",
       "      <td>43146.0</td>\n",
       "      <td>41259.0</td>\n",
       "      <td>82650.0</td>\n",
       "      <td>36883.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Census Tract 5003</td>\n",
       "      <td>26163500300</td>\n",
       "      <td>30881.0</td>\n",
       "      <td>29263.0</td>\n",
       "      <td>59897.0</td>\n",
       "      <td>24115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Census Tract 5004</td>\n",
       "      <td>26163500400</td>\n",
       "      <td>25733.0</td>\n",
       "      <td>23495.0</td>\n",
       "      <td>41176.0</td>\n",
       "      <td>24321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Census Tract 5005</td>\n",
       "      <td>26163500500</td>\n",
       "      <td>37054.0</td>\n",
       "      <td>36075.0</td>\n",
       "      <td>64538.0</td>\n",
       "      <td>31220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>Census Tract 9865</td>\n",
       "      <td>26163986502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>Census Tract 9866</td>\n",
       "      <td>26163986600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>Census Tract 9870</td>\n",
       "      <td>26163987000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>Census Tract 9901</td>\n",
       "      <td>26163990100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>Census Tract 9902</td>\n",
       "      <td>26163990200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>718 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0         Census Tract        GEOID  Median Income (Households)  \\\n",
       "0    Census Tract 5001  26163500100                     36880.0   \n",
       "1    Census Tract 5002  26163500200                     43146.0   \n",
       "2    Census Tract 5003  26163500300                     30881.0   \n",
       "3    Census Tract 5004  26163500400                     25733.0   \n",
       "4    Census Tract 5005  26163500500                     37054.0   \n",
       "..                 ...          ...                         ...   \n",
       "713  Census Tract 9865  26163986502                         NaN   \n",
       "714  Census Tract 9866  26163986600                         NaN   \n",
       "715  Census Tract 9870  26163987000                         NaN   \n",
       "716  Census Tract 9901  26163990100                         NaN   \n",
       "717  Census Tract 9902  26163990200                         NaN   \n",
       "\n",
       "0    Median Income (Families)  Median Income (Married Families)  \\\n",
       "0                     34637.0                           65424.0   \n",
       "1                     41259.0                           82650.0   \n",
       "2                     29263.0                           59897.0   \n",
       "3                     23495.0                           41176.0   \n",
       "4                     36075.0                           64538.0   \n",
       "..                        ...                               ...   \n",
       "713                       NaN                               NaN   \n",
       "714                       NaN                               NaN   \n",
       "715                       NaN                               NaN   \n",
       "716                       NaN                               NaN   \n",
       "717                       NaN                               NaN   \n",
       "\n",
       "0    Median Income (Nonfamily Households)  \n",
       "0                                 34859.0  \n",
       "1                                 36883.0  \n",
       "2                                 24115.0  \n",
       "3                                 24321.0  \n",
       "4                                 31220.0  \n",
       "..                                    ...  \n",
       "713                                   NaN  \n",
       "714                                   NaN  \n",
       "715                                   NaN  \n",
       "716                                   NaN  \n",
       "717                                   NaN  \n",
       "\n",
       "[718 rows x 6 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === INCOME CLEANING FUNCTION ===\n",
    "def clean_income_df(df):\n",
    "    # Set the first row as the header and drop it\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:].copy().reset_index(drop=True)\n",
    "\n",
    "    # Renaming\n",
    "    df = df.rename(columns={\n",
    "        'Geographic Area Name': 'Census Tract',\n",
    "        'Geography': 'GEOID',\n",
    "        'Households!!Estimate!!Median income (dollars)': 'Median Income (Households)',\n",
    "        'Families!!Estimate!!Median income (dollars)': 'Median Income (Families)',\n",
    "        'Married-couple families!!Estimate!!Median income (dollars)': 'Median Income (Married Families)',\n",
    "        'Nonfamily households!!Estimate!!Median income (dollars)': 'Median Income (Nonfamily Households)'\n",
    "    })\n",
    "\n",
    "    # Extracting numeric part from GEOID \n",
    "    df['GEOID'] = df['GEOID'].str.extract(r'US(\\d+)', expand=False)\n",
    "\n",
    "    # Extract only 'Census Tract xxxx'\n",
    "    df['Census Tract'] = df['Census Tract'].str.extract(r'(Census Tract \\d+)', expand=False)\n",
    "\n",
    "    # Columns to keep\n",
    "    keep_cols = [\n",
    "        'GEOID',\n",
    "        'Census Tract',\n",
    "        'Median Income (Households)',\n",
    "        'Median Income (Families)',\n",
    "        'Median Income (Married Families)',\n",
    "        'Median Income (Nonfamily Households)'\n",
    "    ]\n",
    "\n",
    "    # Subset the DataFrame\n",
    "    df = df[keep_cols]\n",
    "\n",
    "    # Replace '-' with NaN and convert median income columns to numeric\n",
    "    for col in keep_cols[2:]:\n",
    "        df[col] = df[col].replace('-', np.nan)\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Drop rows where all median income values are missing\n",
    "    df.dropna(subset=keep_cols[1:], how='all', inplace=True)\n",
    "    df = df.groupby(['Census Tract', 'GEOID'], as_index=False).mean(numeric_only=True).round(0)\n",
    "\n",
    "    # Drop duplicates by GEOID, keeping the first\n",
    "    df = df.drop_duplicates(subset='GEOID', keep='first')\n",
    "\n",
    "    return df\n",
    "\n",
    "cleaned_income_dfs = {}\n",
    "\n",
    "city_income_dfs = {\n",
    "    \"detroit\": acs_dataframes['detroit_income'],\n",
    "    \"philadelphia\": acs_dataframes['philly_income'],\n",
    "    \"pittsburgh\": acs_dataframes['pittsburgh_income'],\n",
    "    \"ct\": acs_dataframes['ct_income']\n",
    "}\n",
    "\n",
    "for city, df in city_income_dfs.items():\n",
    "    cleaned_df = clean_income_df(df)\n",
    "    cleaned_income_dfs[city] = cleaned_df\n",
    "    cleaned_df.to_csv(f\"cleaned_income_{city}.csv\", index=False)\n",
    "\n",
    "# View one of the cleaned DataFrames, for example Detroit:\n",
    "cleaned_income_dfs[\"detroit\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce42487",
   "metadata": {},
   "source": [
    "## Cleaning up the Education ACS data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b0147b7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned_education_detroit.csv\n",
      "Saved cleaned_education_philadelphia.csv\n",
      "Saved cleaned_education_pittsburgh.csv\n",
      "Saved cleaned_education_ct.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Census Tract</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>Less than High School (%)</th>\n",
       "      <th>High School Graduate (%)</th>\n",
       "      <th>Some College or Associate Degree (%)</th>\n",
       "      <th>Bachelor's Degree or Higher (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Census Tract 5001</td>\n",
       "      <td>26163500100</td>\n",
       "      <td>26.1</td>\n",
       "      <td>48.1</td>\n",
       "      <td>24.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Census Tract 5002</td>\n",
       "      <td>26163500200</td>\n",
       "      <td>8.4</td>\n",
       "      <td>39.9</td>\n",
       "      <td>11.9</td>\n",
       "      <td>39.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Census Tract 5003</td>\n",
       "      <td>26163500300</td>\n",
       "      <td>45.8</td>\n",
       "      <td>42.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Census Tract 5004</td>\n",
       "      <td>26163500400</td>\n",
       "      <td>75.9</td>\n",
       "      <td>18.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Census Tract 5005</td>\n",
       "      <td>26163500500</td>\n",
       "      <td>29.7</td>\n",
       "      <td>55.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6740</th>\n",
       "      <td>Census Tract 9864</td>\n",
       "      <td>26163986400</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6741</th>\n",
       "      <td>Census Tract 9865</td>\n",
       "      <td>26163986501</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6742</th>\n",
       "      <td>Census Tract 9865</td>\n",
       "      <td>26163986502</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6743</th>\n",
       "      <td>Census Tract 9866</td>\n",
       "      <td>26163986600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6744</th>\n",
       "      <td>Census Tract 9870</td>\n",
       "      <td>26163987000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>719 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0          Census Tract        GEOID Less than High School (%)  \\\n",
       "0     Census Tract 5001  26163500100                      26.1   \n",
       "1     Census Tract 5002  26163500200                       8.4   \n",
       "2     Census Tract 5003  26163500300                      45.8   \n",
       "3     Census Tract 5004  26163500400                      75.9   \n",
       "4     Census Tract 5005  26163500500                      29.7   \n",
       "...                 ...          ...                       ...   \n",
       "6740  Census Tract 9864  26163986400                         0   \n",
       "6741  Census Tract 9865  26163986501                         0   \n",
       "6742  Census Tract 9865  26163986502                         0   \n",
       "6743  Census Tract 9866  26163986600                         0   \n",
       "6744  Census Tract 9870  26163987000                         0   \n",
       "\n",
       "0    High School Graduate (%) Some College or Associate Degree (%)  \\\n",
       "0                        48.1                                 24.1   \n",
       "1                        39.9                                 11.9   \n",
       "2                        42.3                                  8.4   \n",
       "3                        18.6                                  5.5   \n",
       "4                        55.0                                 15.3   \n",
       "...                       ...                                  ...   \n",
       "6740                       15                                    0   \n",
       "6741                        0                                    0   \n",
       "6742                        0                                    0   \n",
       "6743                        0                                    0   \n",
       "6744                        0                                    0   \n",
       "\n",
       "0    Bachelor's Degree or Higher (%)  \n",
       "0                                1.8  \n",
       "1                               39.9  \n",
       "2                                3.5  \n",
       "3                                0.0  \n",
       "4                                0.0  \n",
       "...                              ...  \n",
       "6740                               0  \n",
       "6741                               0  \n",
       "6742                               0  \n",
       "6743                               0  \n",
       "6744                               0  \n",
       "\n",
       "[719 rows x 6 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === EDUCATION CLEANING FUNCTION ===\n",
    "def clean_education_df(df):\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:].copy().reset_index(drop=True)\n",
    "\n",
    "    # Renaming columns\n",
    "    df = df.rename(columns={\n",
    "        'Geographic Area Name': 'Census Tract',\n",
    "        'Geography': 'GEOID',\n",
    "        'Total!!Estimate!!Less than high school graduate': 'Less than High School (%)',\n",
    "        'Total!!Estimate!!High school graduate (includes equivalency)': 'High School Graduate (%)',\n",
    "        \"Total!!Estimate!!Some college or associate's degree\": 'Some College or Associate Degree (%)',\n",
    "        \"Total!!Estimate!!Bachelor's degree or higher\": \"Bachelor's Degree or Higher (%)\"\n",
    "    })\n",
    "\n",
    "    # Extract numeric part from GEOID (everything after 'US')\n",
    "    df['GEOID'] = df['GEOID'].str.extract(r'US(\\d+)', expand=False)\n",
    "\n",
    "    # Extracting numeric part from Census Tract\n",
    "    df['Census Tract'] = df['Census Tract'].str.extract(r'(Census Tract \\d+)', expand=False)\n",
    "\n",
    "    keep_cols = [\n",
    "        'Census Tract',\n",
    "        'GEOID',\n",
    "        'Less than High School (%)',\n",
    "        'High School Graduate (%)',\n",
    "        'Some College or Associate Degree (%)',\n",
    "        \"Bachelor's Degree or Higher (%)\"\n",
    "    ]\n",
    "    df = df[keep_cols]\n",
    "\n",
    "    # Drop duplicates by GEOID, keeping the first\n",
    "    df = df.drop_duplicates(subset='GEOID', keep='first')\n",
    "\n",
    "    return df\n",
    "\n",
    "# === PROCESS EDUCATION DATA ===\n",
    "cleaned_education_dfs = {}\n",
    "city_education_dfs = {\n",
    "    \"detroit\": acs_dataframes['detroit_ed_attainment'],\n",
    "    \"philadelphia\": acs_dataframes['philly_ed_attainment'],\n",
    "    \"pittsburgh\": acs_dataframes['pittsburgh_ed_attainment'],\n",
    "    \"ct\": acs_dataframes['ct_ed_attainment']\n",
    "}\n",
    "for city, df in city_education_dfs.items():\n",
    "    cleaned_df = clean_education_df(df)\n",
    "    cleaned_education_dfs[city] = cleaned_df\n",
    "    cleaned_df.to_csv(f\"cleaned_education_{city}.csv\", index=False)\n",
    "    print(f\"Saved cleaned_education_{city}.csv\")\n",
    "\n",
    "# View one cleaned education DataFrame, for example Detroit:\n",
    "cleaned_education_dfs[\"detroit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292b376",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Checking how many census tracts match between Education and Income data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55726d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "City: Detroit\n",
      "  Tracts only in income data: []\n",
      "  Tracts only in education data: []\n",
      "  Matching tracts: 661 / 661 total unique tracts\n",
      "\n",
      "City: Philadelphia\n",
      "  Tracts only in income data: []\n",
      "  Tracts only in education data: []\n",
      "  Matching tracts: 330 / 330 total unique tracts\n",
      "\n",
      "City: Pittsburgh\n",
      "  Tracts only in income data: []\n",
      "  Tracts only in education data: []\n",
      "  Matching tracts: 378 / 378 total unique tracts\n",
      "\n",
      "City: Ct\n",
      "  Tracts only in income data: []\n",
      "  Tracts only in education data: []\n",
      "  Matching tracts: 1342 / 1342 total unique tracts\n"
     ]
    }
   ],
   "source": [
    "for city in cleaned_income_dfs.keys():\n",
    "    income_tracts = set(cleaned_income_dfs[city]['Census Tract'].dropna().unique())\n",
    "    ed_tracts = set(cleaned_education_dfs[city]['Census Tract'].dropna().unique())\n",
    "\n",
    "    only_in_income = income_tracts - ed_tracts\n",
    "    only_in_ed = ed_tracts - income_tracts\n",
    "\n",
    "    print(f\"\\nCity: {city.capitalize()}\")\n",
    "    print(f\"  Tracts only in income data: {sorted(only_in_income)}\")\n",
    "    print(f\"  Tracts only in education data: {sorted(only_in_ed)}\")\n",
    "    print(f\"  Matching tracts: {len(income_tracts & ed_tracts)} / {len(income_tracts | ed_tracts)} total unique tracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e9226",
   "metadata": {},
   "source": [
    "## Cleaning up race ACS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fff100aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned_race_detroit.csv\n",
      "Saved cleaned_race_philadelphia.csv\n",
      "Saved cleaned_race_pittsburgh.csv\n",
      "Saved cleaned_race_ct.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEOID</th>\n",
       "      <th>Census Tract</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>White (%)</th>\n",
       "      <th>Black (%)</th>\n",
       "      <th>Asian (%)</th>\n",
       "      <th>Other (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.616350e+10</td>\n",
       "      <td>Census Tract 5001</td>\n",
       "      <td>3557.0</td>\n",
       "      <td>5.847624</td>\n",
       "      <td>93.533877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.265111e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.616350e+10</td>\n",
       "      <td>Census Tract 5002</td>\n",
       "      <td>3045.0</td>\n",
       "      <td>8.801314</td>\n",
       "      <td>83.809524</td>\n",
       "      <td>3.973727</td>\n",
       "      <td>6.896552e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.616350e+10</td>\n",
       "      <td>Census Tract 5003</td>\n",
       "      <td>3225.0</td>\n",
       "      <td>7.255814</td>\n",
       "      <td>90.945736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.503876e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.616350e+10</td>\n",
       "      <td>Census Tract 5004</td>\n",
       "      <td>1628.0</td>\n",
       "      <td>4.361179</td>\n",
       "      <td>95.638821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.457002e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.616350e+10</td>\n",
       "      <td>Census Tract 5005</td>\n",
       "      <td>2579.0</td>\n",
       "      <td>2.675456</td>\n",
       "      <td>91.314463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.483521e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6740</th>\n",
       "      <td>2.616399e+10</td>\n",
       "      <td>Census Tract 9864</td>\n",
       "      <td>45.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.498000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6741</th>\n",
       "      <td>2.616399e+10</td>\n",
       "      <td>Census Tract 9865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6742</th>\n",
       "      <td>2.616399e+10</td>\n",
       "      <td>Census Tract 9865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6743</th>\n",
       "      <td>2.616399e+10</td>\n",
       "      <td>Census Tract 9866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6744</th>\n",
       "      <td>2.616399e+10</td>\n",
       "      <td>Census Tract 9870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>719 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0            GEOID       Census Tract  Total Population  White (%)  Black (%)  \\\n",
       "0     2.616350e+10  Census Tract 5001            3557.0   5.847624  93.533877   \n",
       "1     2.616350e+10  Census Tract 5002            3045.0   8.801314  83.809524   \n",
       "2     2.616350e+10  Census Tract 5003            3225.0   7.255814  90.945736   \n",
       "3     2.616350e+10  Census Tract 5004            1628.0   4.361179  95.638821   \n",
       "4     2.616350e+10  Census Tract 5005            2579.0   2.675456  91.314463   \n",
       "...            ...                ...               ...        ...        ...   \n",
       "6740  2.616399e+10  Census Tract 9864              45.0  66.666667  33.333333   \n",
       "6741  2.616399e+10  Census Tract 9865               0.0        NaN        NaN   \n",
       "6742  2.616399e+10  Census Tract 9865               0.0        NaN        NaN   \n",
       "6743  2.616399e+10  Census Tract 9866               0.0        NaN        NaN   \n",
       "6744  2.616399e+10  Census Tract 9870               0.0        NaN        NaN   \n",
       "\n",
       "0     Asian (%)     Other (%)  \n",
       "0      0.000000  1.265111e+00  \n",
       "1      3.973727  6.896552e+00  \n",
       "2      0.000000  3.503876e+00  \n",
       "3      0.000000  2.457002e-01  \n",
       "4      0.000000  7.483521e+00  \n",
       "...         ...           ...  \n",
       "6740   0.000000  1.498000e+04  \n",
       "6741        NaN           inf  \n",
       "6742        NaN           inf  \n",
       "6743        NaN           inf  \n",
       "6744        NaN           inf  \n",
       "\n",
       "[719 rows x 7 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === RACE CLEANING FUNCTION ===\n",
    "def clean_race_df(df):\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:].copy().reset_index(drop=True)\n",
    "\n",
    "    # Renaming \n",
    "    df = df.rename(columns={\n",
    "        'Geographic Area Name': 'Census Tract',\n",
    "        'Geography': 'GEOID',\n",
    "        'Estimate!!Total': 'Total Population'\n",
    "    })\n",
    "\n",
    "    # Extract GEOID from the GEO_ID column\n",
    "    df['GEOID'] = df['GEOID'].str.extract(r'US(\\d+)', expand=False)\n",
    "\n",
    "    # Extract only 'Census Tract xxxx'\n",
    "    df['Census Tract'] = df['Census Tract'].str.extract(r'(Census Tract \\d+)', expand=False)\n",
    "\n",
    "    # Convert all relevant race columns to numeric\n",
    "    for col in df.columns:\n",
    "        if col != 'Census Tract':\n",
    "            df[col] = df[col].replace('-', np.nan)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    keep = [\n",
    "        'GEOID',\n",
    "        'Census Tract',\n",
    "        'Total Population',\n",
    "        'Estimate!!Total!!White alone',\n",
    "        'Estimate!!Total!!Black or African American alone',\n",
    "        'Estimate!!Total!!Asian alone'\n",
    "    ]\n",
    "    race_columns_to_sum = [col for col in df.columns if col not in keep and col != 'Census Tract']\n",
    "    df['Estimate!!Total!!Others'] = df[race_columns_to_sum].sum(axis=1)\n",
    "\n",
    "    # Convert race totals to percentages\n",
    "    df['White (%)'] = (df['Estimate!!Total!!White alone'] / df['Total Population']) * 100\n",
    "    df['Black (%)'] = (df['Estimate!!Total!!Black or African American alone'] / df['Total Population']) * 100\n",
    "    df['Asian (%)'] = (df['Estimate!!Total!!Asian alone'] / df['Total Population']) * 100\n",
    "    df['Other (%)'] = (df['Estimate!!Total!!Others'] / df['Total Population']) * 100\n",
    "\n",
    "    # Drop raw total race counts\n",
    "    df = df.drop(columns=[\n",
    "        'Estimate!!Total!!White alone',\n",
    "        'Estimate!!Total!!Black or African American alone',\n",
    "        'Estimate!!Total!!Asian alone',\n",
    "        'Estimate!!Total!!Others'\n",
    "    ])\n",
    "\n",
    "    df = df[['GEOID', 'Census Tract', 'Total Population', 'White (%)', 'Black (%)', 'Asian (%)', 'Other (%)']]\n",
    "\n",
    "    # Drop duplicates by GEOID, keeping the first\n",
    "    df = df.drop_duplicates(subset='GEOID', keep='first')\n",
    "\n",
    "    return df\n",
    "\n",
    "# === PROCESS RACE DATA ===\n",
    "cleaned_race_dfs = {}\n",
    "city_race_dfs = {\n",
    "    \"detroit\": acs_dataframes['detroit_race'],\n",
    "    \"philadelphia\": acs_dataframes['philly_race'],\n",
    "    \"pittsburgh\": acs_dataframes['pittsburgh_race'],\n",
    "    \"ct\": acs_dataframes['ct_race']\n",
    "}\n",
    "for city, df in city_race_dfs.items():\n",
    "    cleaned_df = clean_race_df(df)\n",
    "    cleaned_race_dfs[city] = cleaned_df\n",
    "    cleaned_df.to_csv(f\"cleaned_race_{city}.csv\", index=False)\n",
    "    print(f\"Saved cleaned_race_{city}.csv\")\n",
    "\n",
    "# View one cleaned race DataFrame, for example Detroit:\n",
    "cleaned_race_dfs[\"detroit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3abe2",
   "metadata": {},
   "source": [
    "## Cleaning up employment ACS data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7b4c1cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned_employment_detroit.csv\n",
      "Saved cleaned_employment_philadelphia.csv\n",
      "Saved cleaned_employment_pittsburgh.csv\n",
      "Saved cleaned_employment_ct.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEOID</th>\n",
       "      <th>Census Tract</th>\n",
       "      <th>Unemployment Rate (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26163500100</td>\n",
       "      <td>Census Tract 5001</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26163500200</td>\n",
       "      <td>Census Tract 5002</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26163500300</td>\n",
       "      <td>Census Tract 5003</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26163500400</td>\n",
       "      <td>Census Tract 5004</td>\n",
       "      <td>34.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26163500500</td>\n",
       "      <td>Census Tract 5005</td>\n",
       "      <td>36.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6691</th>\n",
       "      <td>26163595201</td>\n",
       "      <td>Census Tract 5952</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6697</th>\n",
       "      <td>26163599001</td>\n",
       "      <td>Census Tract 5990</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6718</th>\n",
       "      <td>26163983400</td>\n",
       "      <td>Census Tract 9834</td>\n",
       "      <td>11.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6740</th>\n",
       "      <td>26163986400</td>\n",
       "      <td>Census Tract 9864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8587</th>\n",
       "      <td>26163982000</td>\n",
       "      <td>Census Tract 9820</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>678 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0           GEOID       Census Tract  Unemployment Rate (%)\n",
       "0     26163500100  Census Tract 5001                   25.6\n",
       "1     26163500200  Census Tract 5002                   18.0\n",
       "2     26163500300  Census Tract 5003                   32.0\n",
       "3     26163500400  Census Tract 5004                   34.8\n",
       "4     26163500500  Census Tract 5005                   36.7\n",
       "...           ...                ...                    ...\n",
       "6691  26163595201  Census Tract 5952                    2.7\n",
       "6697  26163599001  Census Tract 5990                    3.9\n",
       "6718  26163983400  Census Tract 9834                   11.6\n",
       "6740  26163986400  Census Tract 9864                    0.0\n",
       "8587  26163982000  Census Tract 9820                    0.0\n",
       "\n",
       "[678 rows x 3 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === EMPLOYMENT CLEANING FUNCTION ===\n",
    "def clean_employment_df(df):\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:].copy().reset_index(drop=True)\n",
    "    \n",
    "    # Renaming\n",
    "    df = df.rename(columns={\n",
    "        'Geographic Area Name': 'Census Tract',\n",
    "        'Geography': 'GEOID',\n",
    "        'Unemployment rate!!Estimate!!Population 16 years and over': 'Unemployment Rate (%)'\n",
    "    })\n",
    "\n",
    "    # Extract GEOID from the GEO_ID column\n",
    "    df['GEOID'] = df['GEOID'].str.extract(r'US(\\d+)', expand=False)\n",
    "\n",
    "    # Extract Census Tract\n",
    "    df['Census Tract'] = df['Census Tract'].str.extract(r'(Census Tract \\d+)', expand=False)\n",
    "\n",
    "    # Keep only GEOID, Census Tract, and Unemployment Rate\n",
    "    df = df[['GEOID', 'Census Tract', 'Unemployment Rate (%)']]\n",
    "    df['Unemployment Rate (%)'] = df['Unemployment Rate (%)'].replace('-', np.nan)\n",
    "    df['Unemployment Rate (%)'] = pd.to_numeric(df['Unemployment Rate (%)'], errors='coerce')\n",
    "    df.dropna(subset=['Unemployment Rate (%)'], how='all', inplace=True)\n",
    "\n",
    "    # Drop duplicates by GEOID, keeping the first\n",
    "    # Duplicates exist by GEOID, so we just kept only the first values \n",
    "    df = df.drop_duplicates(subset='GEOID', keep='first')\n",
    "\n",
    "    return df\n",
    "\n",
    "# === PROCESS EMPLOYMENT DATA ===\n",
    "cleaned_employment_dfs = {}\n",
    "city_employment_dfs = {\n",
    "    \"detroit\": acs_dataframes['detroit_employment'],\n",
    "    \"philadelphia\": acs_dataframes['philly_employment'],\n",
    "    \"pittsburgh\": acs_dataframes['pittsburgh_employment'],\n",
    "    \"ct\": acs_dataframes['ct_employment']\n",
    "}\n",
    "for city, df in city_employment_dfs.items():\n",
    "    cleaned_df = clean_employment_df(df)\n",
    "    cleaned_employment_dfs[city] = cleaned_df\n",
    "    cleaned_df.to_csv(f\"cleaned_employment_{city}.csv\", index=False)\n",
    "    print(f\"Saved cleaned_employment_{city}.csv\")\n",
    "\n",
    "# View one cleaned employment DataFrame, for example Detroit:\n",
    "cleaned_employment_dfs[\"detroit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56eb208",
   "metadata": {},
   "source": [
    "### How many tracts intersect with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cff28c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "City: Detroit\n",
      "  Total Census Tracts in Income: 661\n",
      "  Total Census Tracts in Education: 661\n",
      "  Total Census Tracts in Race: 661\n",
      "  Total Census Tracts in Employment: 628\n",
      "  ➤ Tracts present in ALL four datasets: 628\n",
      "\n",
      "City: Philadelphia\n",
      "  Total Census Tracts in Income: 330\n",
      "  Total Census Tracts in Education: 330\n",
      "  Total Census Tracts in Race: 330\n",
      "  Total Census Tracts in Employment: 321\n",
      "  ➤ Tracts present in ALL four datasets: 321\n",
      "\n",
      "City: Pittsburgh\n",
      "  Total Census Tracts in Income: 378\n",
      "  Total Census Tracts in Education: 378\n",
      "  Total Census Tracts in Race: 378\n",
      "  Total Census Tracts in Employment: 375\n",
      "  ➤ Tracts present in ALL four datasets: 375\n",
      "\n",
      "City: Ct\n",
      "  Total Census Tracts in Income: 1342\n",
      "  Total Census Tracts in Education: 1342\n",
      "  Total Census Tracts in Race: 756\n",
      "  Total Census Tracts in Employment: 1307\n",
      "  ➤ Tracts present in ALL four datasets: 753\n"
     ]
    }
   ],
   "source": [
    "# === TRACT INTERSECTION REPORT ===\n",
    "for city in cleaned_income_dfs.keys():\n",
    "    income_tracts = set(cleaned_income_dfs[city]['Census Tract'].dropna())\n",
    "    edu_tracts = set(cleaned_education_dfs[city]['Census Tract'].dropna())\n",
    "    race_tracts = set(cleaned_race_dfs[city]['Census Tract'].dropna())\n",
    "    emp_tracts = set(cleaned_employment_dfs[city]['Census Tract'].dropna())\n",
    "\n",
    "    common_tracts = income_tracts & edu_tracts & race_tracts & emp_tracts\n",
    "\n",
    "    print(f\"\\nCity: {city.capitalize()}\")\n",
    "    print(f\"  Total Census Tracts in Income: {len(income_tracts)}\")\n",
    "    print(f\"  Total Census Tracts in Education: {len(edu_tracts)}\")\n",
    "    print(f\"  Total Census Tracts in Race: {len(race_tracts)}\")\n",
    "    print(f\"  Total Census Tracts in Employment: {len(emp_tracts)}\")\n",
    "    print(f\"  ➤ Tracts present in ALL four datasets: {len(common_tracts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78575c",
   "metadata": {},
   "source": [
    "### Combining dfs into one per city "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d9f0602c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved compiled_detroit_acs.csv with 678 tracts\n",
      "Saved compiled_philadelphia_acs.csv with 408 tracts\n",
      "Saved compiled_pittsburgh_acs.csv with 426 tracts\n",
      "Saved compiled_ct_acs.csv with 1807 tracts\n"
     ]
    }
   ],
   "source": [
    "# === COMBINE CLEANED DATASETS PER CITY ===\n",
    "combined_city_dfs = {}\n",
    "\n",
    "for city in cleaned_income_dfs:\n",
    "    income = cleaned_income_dfs[city].copy()\n",
    "    edu = cleaned_education_dfs[city].copy()\n",
    "    race = cleaned_race_dfs[city].copy()\n",
    "    emp = cleaned_employment_dfs[city].copy()\n",
    "\n",
    "    # Normalize GEOID formatting\n",
    "    for df in [income, edu, race, emp]:\n",
    "        df['GEOID'] = df['GEOID'].astype(str).str.extract(r'(\\d+)', expand=False).str.strip().str.zfill(11)\n",
    "\n",
    "    # Find shared GEOIDs\n",
    "    common_geoids = set(income['GEOID']) & set(edu['GEOID']) & set(race['GEOID']) & set(emp['GEOID'])\n",
    "\n",
    "    # Filter each dataframe to only common GEOIDs\n",
    "    income = income[income['GEOID'].isin(common_geoids)]\n",
    "    edu = edu[edu['GEOID'].isin(common_geoids)]\n",
    "    race = race[race['GEOID'].isin(common_geoids)]\n",
    "    emp = emp[emp['GEOID'].isin(common_geoids)]\n",
    "\n",
    "    # Drop 'Census Tract' from all but one dataframe to avoid merge conflicts\n",
    "    edu = edu.drop(columns=['Census Tract'], errors='ignore')\n",
    "    race = race.drop(columns=['Census Tract'], errors='ignore')\n",
    "    emp = emp.drop(columns=['Census Tract'], errors='ignore')\n",
    "\n",
    "    # Merge them all on 'GEOID'\n",
    "    merged = income.merge(edu, on='GEOID')\\\n",
    "                   .merge(race, on='GEOID')\\\n",
    "                   .merge(emp, on='GEOID')\n",
    "\n",
    "    combined_city_dfs[city] = merged\n",
    "    merged.to_csv(f\"compiled_{city}_acs.csv\", index=False)\n",
    "    print(f\"Saved compiled_{city}_acs.csv with {len(merged)} tracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd37d60",
   "metadata": {},
   "source": [
    "## Reading in Processed commerce data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aae51949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imported the following commerce data files:\n",
      "  - processed_ct_commerce\n",
      "  - processed_philly_commerce\n",
      "  - processed_detroit_commerce\n",
      "  - processed_pitt_commerce\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the parent folder containing all subfolders\n",
    "parent_folder = '/Users/junsik/Documents/Dickinson College/Spring 2025/DATA 400/Final Project/Data/OneDrive Apr 7 2025/processed_commerce_data'\n",
    "\n",
    "# Create an empty dictionary to store DataFrames\n",
    "compiled_commerce_dataframes = {}\n",
    "\n",
    "# Track imported files\n",
    "imported_files = []\n",
    "\n",
    "# Loop through all subfolders and files in the parent folder\n",
    "for root, dirs, files in os.walk(parent_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            df_name = os.path.splitext(file)[0]\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Standardize GEOID if the column exists\n",
    "            if 'GEOID_TRACT_20' in df.columns:\n",
    "                df = df.rename(columns={'GEOID_TRACT_20': 'GEOID'})\n",
    "            elif 'tract' in df.columns:\n",
    "                df = df.rename(columns={'tract': 'GEOID'})\n",
    "\n",
    "            if 'GEOID' in df.columns:\n",
    "                df['GEOID'] = df['GEOID'].astype(str).str.extract(r'(\\d+)', expand=False).str.strip().str.zfill(11)\n",
    "\n",
    "            # Keep only GEOID and total_businesses columns\n",
    "            keep_cols = ['GEOID', 'total_businesses']\n",
    "            df = df[[col for col in df.columns if col in keep_cols]]\n",
    "            \n",
    "            # Rename for readability\n",
    "            df = df.rename(columns={'total_businesses': 'Total Businesses'})\n",
    "\n",
    "            compiled_commerce_dataframes[df_name] = df\n",
    "            imported_files.append(df_name)\n",
    "\n",
    "# Print summary of imported files\n",
    "print(\"✅ Imported the following commerce data files:\")\n",
    "for name in imported_files:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079e6a6",
   "metadata": {},
   "source": [
    "## Merging ACS with Commerce Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8c27cc20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged Detroit — 289 tracts saved.\n",
      "✅ Merged Philadelphia — 382 tracts saved.\n",
      "✅ Merged Pittsburgh — 204 tracts saved.\n",
      "✅ Merged Ct — 874 tracts saved.\n"
     ]
    }
   ],
   "source": [
    "# Map between ACS city keys and their corresponding commerce DataFrame names\n",
    "commerce_key_map = {\n",
    "    \"detroit\": \"processed_detroit_commerce\",\n",
    "    \"philadelphia\": \"processed_philly_commerce\",\n",
    "    \"pittsburgh\": \"processed_pitt_commerce\",\n",
    "    \"ct\": \"processed_ct_commerce\"\n",
    "}\n",
    "\n",
    "merged_acs_commerce_dfs = {}\n",
    "\n",
    "for city, acs_df in combined_city_dfs.items():\n",
    "    acs_df = acs_df.copy()\n",
    "    commerce_key = commerce_key_map.get(city)\n",
    "\n",
    "    if commerce_key in compiled_commerce_dataframes:\n",
    "        commerce_df = compiled_commerce_dataframes[commerce_key].copy()\n",
    "\n",
    "        # Merge\n",
    "        merged = acs_df.merge(commerce_df, on=\"GEOID\", how=\"inner\")\n",
    "        merged_acs_commerce_dfs[city] = merged\n",
    "\n",
    "        # Save to CSV\n",
    "        merged.to_csv(f\"merged_{city}_acs_commerce.csv\", index=False)\n",
    "        print(f\"✅ Merged {city.capitalize()} — {len(merged)} tracts saved.\")\n",
    "    else:\n",
    "        print(f\"⚠️  No commerce data found for {city}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b68f22",
   "metadata": {},
   "source": [
    "## Reading in processed crime data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1782afe2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: new_processed_pitt_crime\n",
      "Processing: new_processed_phl\n",
      "Processing: new_processed_det\n",
      "⏭️ Skipping: processed_ct copy\n",
      "\n",
      "✅ Imported and cleaned the following crime data files:\n",
      "  - new_processed_pitt_crime\n",
      "  - new_processed_phl\n",
      "  - new_processed_det\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the parent folder containing all subfolders\n",
    "parent_folder = '/Users/junsik/Documents/Dickinson College/Spring 2025/DATA 400/Final Project/Data/OneDrive Apr 7 2025/Processed Crime Data'\n",
    "\n",
    "# Cleaned and averaged crime data\n",
    "cleaned_crime_dataframes = {}\n",
    "\n",
    "# Track imported files\n",
    "imported_files = []\n",
    "\n",
    "# Read in files first\n",
    "for root, dirs, files in os.walk(parent_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            df_name = os.path.splitext(file)[0]\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Skip Connecticut data if present\n",
    "            if 'ct' in df_name.lower():\n",
    "                print(f\"\\u23ED\\ufe0f Skipping: {df_name}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing: {df_name}\")\n",
    "\n",
    "            # Standardize GEOID\n",
    "            if 'GEOID' in df.columns:\n",
    "                pass  # it's already named GEOID\n",
    "            elif 'geoid' in df.columns:\n",
    "                df = df.rename(columns={'geoid': 'GEOID'})\n",
    "            elif 'geo_id' in df.columns:\n",
    "                df = df.rename(columns={'geo_id': 'GEOID'})\n",
    "            else:\n",
    "                print(f\"⚠️ Skipping {df_name} because no GEOID-related column found.\")\n",
    "                continue\n",
    "\n",
    "            # Now standardize depending on city\n",
    "            if 'det' in df_name.lower():\n",
    "                # Detroit: special fix\n",
    "                df['GEOID'] = df['GEOID'].astype(str)\n",
    "                df['GEOID'] = df['GEOID'].str.split('.').str[0]\n",
    "                df['GEOID'] = df['GEOID'].str.slice(0, 11)\n",
    "                df['GEOID'] = df['GEOID'].str.zfill(11)\n",
    "            else:\n",
    "                # Normal cases\n",
    "                df['GEOID'] = df['GEOID'].astype(str).str.extract(r'(\\d+)', expand=False).str.zfill(11)\n",
    "\n",
    "            # Drop year columns and unnamed columns\n",
    "            year_cols = [col for col in df.columns if 'year' in col.lower() or col.startswith('Unnamed')]\n",
    "            df = df.drop(columns=year_cols, errors='ignore')\n",
    "\n",
    "            # Keep only numeric columns and GEOID\n",
    "            numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "            if 'GEOID' not in numeric_cols:\n",
    "                numeric_cols.append('GEOID')\n",
    "            df = df[numeric_cols]\n",
    "\n",
    "            # Group by GEOID and average\n",
    "            df_avg = df.groupby('GEOID', as_index=False).mean().round(2)\n",
    "\n",
    "            # Rename crime columns\n",
    "            df_avg = df_avg.rename(columns={\n",
    "                'nuisance_crime': 'Nuisance Crime Rate',\n",
    "                'other_crime': 'Other Crime Rate',\n",
    "                'property_crime': 'Property Crime Rate',\n",
    "                'violent_crime': 'Violent Crime Rate',\n",
    "                'total_offenses': 'Total Crime Rate'\n",
    "            })\n",
    "\n",
    "            cleaned_crime_dataframes[df_name] = df_avg\n",
    "            imported_files.append(df_name)\n",
    "\n",
    "# Print imported files\n",
    "print(\"\\n✅ Imported and cleaned the following crime data files:\")\n",
    "for name in imported_files:\n",
    "    print(f\"  - {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de68df",
   "metadata": {},
   "source": [
    "## Merging Crime data with ACS and Commerce Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9f67ad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged Detroit with ACS + Commerce + Crime — 265 tracts saved.\n",
      "✅ Merged Philadelphia with ACS + Commerce + Crime — 351 tracts saved.\n",
      "✅ Merged Pittsburgh with ACS + Commerce + Crime — 95 tracts saved.\n",
      "⚠️  No crime data found for ct\n"
     ]
    }
   ],
   "source": [
    "# === Merge crime data into merged ACS + Commerce data ===\n",
    "crime_key_map = {\n",
    "    \"philadelphia\": \"new_processed_phl\",\n",
    "    \"pittsburgh\": \"new_processed_pitt_crime\",\n",
    "    \"detroit\": \"new_processed_det\"\n",
    "}\n",
    "\n",
    "merged_acs_commerce_crime_dfs = {}\n",
    "\n",
    "for city, acs_commerce_df in merged_acs_commerce_dfs.items():\n",
    "    crime_key = crime_key_map.get(city)\n",
    "\n",
    "    if crime_key and crime_key in cleaned_crime_dataframes:\n",
    "        crime_df = cleaned_crime_dataframes[crime_key].copy()\n",
    "\n",
    "        # Merge on GEOID\n",
    "        merged = acs_commerce_df.merge(crime_df, on='GEOID', how='inner')\n",
    "        merged_acs_commerce_crime_dfs[city] = merged\n",
    "\n",
    "        # Save result\n",
    "        merged.to_csv(f\"merged_{city}_acs_commerce_crime.csv\", index=False)\n",
    "        print(f\"✅ Merged {city.capitalize()} with ACS + Commerce + Crime — {len(merged)} tracts saved.\")\n",
    "    else:\n",
    "        print(f\"⚠️  No crime data found for {city}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f53509",
   "metadata": {},
   "source": [
    "## Reading in Sales Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7b3f0d42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_c/t_p4w1f91_11yj717hx6qttc0000gn/T/ipykernel_98628/1348541265.py:2: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sales = pd.read_csv('/Users/junsik/Documents/Dickinson College/Spring 2025/DATA 400/Final Project/Data/OneDrive Apr 7 2025/full_merged_sales.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>City</th>\n",
       "      <th>Address</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>parcel_id</th>\n",
       "      <th>Location</th>\n",
       "      <th>Sale Date</th>\n",
       "      <th>Sale Price</th>\n",
       "      <th>GEOID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>1841 OAKMONT ST</td>\n",
       "      <td>19111</td>\n",
       "      <td>50016134.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-05 05:00:00+00:00</td>\n",
       "      <td>88000.0</td>\n",
       "      <td>42101030501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>1841 OAKMONT ST</td>\n",
       "      <td>19111</td>\n",
       "      <td>50016134.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-05 05:00:00+00:00</td>\n",
       "      <td>88000.0</td>\n",
       "      <td>42101030502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>1841 OAKMONT ST</td>\n",
       "      <td>19111</td>\n",
       "      <td>50016134.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-05 05:00:00+00:00</td>\n",
       "      <td>88000.0</td>\n",
       "      <td>42101030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>1841 OAKMONT ST</td>\n",
       "      <td>19111</td>\n",
       "      <td>50016134.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-05 05:00:00+00:00</td>\n",
       "      <td>88000.0</td>\n",
       "      <td>42101030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>1841 OAKMONT ST</td>\n",
       "      <td>19111</td>\n",
       "      <td>50016134.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-05 05:00:00+00:00</td>\n",
       "      <td>88000.0</td>\n",
       "      <td>42101030800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          City          Address  Zip Code   parcel_id Location  \\\n",
       "0           0  PHILADELPHIA  1841 OAKMONT ST     19111  50016134.0      NaN   \n",
       "1           1  PHILADELPHIA  1841 OAKMONT ST     19111  50016134.0      NaN   \n",
       "2           2  PHILADELPHIA  1841 OAKMONT ST     19111  50016134.0      NaN   \n",
       "3           3  PHILADELPHIA  1841 OAKMONT ST     19111  50016134.0      NaN   \n",
       "4           4  PHILADELPHIA  1841 OAKMONT ST     19111  50016134.0      NaN   \n",
       "\n",
       "                   Sale Date  Sale Price        GEOID  \n",
       "0  2000-01-05 05:00:00+00:00     88000.0  42101030501  \n",
       "1  2000-01-05 05:00:00+00:00     88000.0  42101030502  \n",
       "2  2000-01-05 05:00:00+00:00     88000.0  42101030600  \n",
       "3  2000-01-05 05:00:00+00:00     88000.0  42101030700  \n",
       "4  2000-01-05 05:00:00+00:00     88000.0  42101030800  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Sales Data  \n",
    "sales = pd.read_csv('/Users/junsik/Documents/Dickinson College/Spring 2025/DATA 400/Final Project/Data/OneDrive Apr 7 2025/full_merged_sales.csv')\n",
    "\n",
    "# Rename and standardize columns\n",
    "sales = sales.rename(columns={\n",
    "    'geo_id': 'GEOID',\n",
    "    'zip_code': 'Zip Code',\n",
    "    'sale_date': 'Sale Date',\n",
    "    'sale_price': 'Sale Price',\n",
    "    'city': 'City',\n",
    "    'address': 'Address',\n",
    "    'location': 'Location'\n",
    "})\n",
    "\n",
    "# Format columns\n",
    "sales['GEOID'] = sales['GEOID'].astype(str).str.zfill(11)\n",
    "sales['Zip Code'] = sales['Zip Code'].astype(int)\n",
    "\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77799001",
   "metadata": {},
   "source": [
    "## How many GEOID overlap with Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "41d704a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detroit:\n",
      "  GEOIDs in ACS+Commerce: 265\n",
      "  GEOIDs in Sales Data: 1176\n",
      "  ➤ Overlapping GEOIDs: 265\n",
      "\n",
      "Philadelphia:\n",
      "  GEOIDs in ACS+Commerce: 351\n",
      "  GEOIDs in Sales Data: 1176\n",
      "  ➤ Overlapping GEOIDs: 351\n",
      "\n",
      "Pittsburgh:\n",
      "  GEOIDs in ACS+Commerce: 95\n",
      "  GEOIDs in Sales Data: 1176\n",
      "  ➤ Overlapping GEOIDs: 95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure sales GEOID is clean and 11 digits\n",
    "sales['GEOID'] = sales['GEOID'].astype(str).str.zfill(11)\n",
    "sales_geoids = set(sales['GEOID'].dropna())\n",
    "\n",
    "# Check overlap with each city in merged_acs_commerce_dfs\n",
    "for city, df in merged_acs_commerce_crime_dfs.items():\n",
    "    city_geoids = set(df['GEOID'].dropna())\n",
    "    common = sales_geoids & city_geoids\n",
    "\n",
    "    print(f\"{city.capitalize()}:\")\n",
    "    print(f\"  GEOIDs in ACS+Commerce: {len(city_geoids)}\")\n",
    "    print(f\"  GEOIDs in Sales Data: {len(sales_geoids)}\")\n",
    "    print(f\"  ➤ Overlapping GEOIDs: {len(common)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8968d87",
   "metadata": {},
   "source": [
    "## Merging Sales data for Philadelphia, Detroit, and Pittsburgh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "14e93562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged Philadelphia with Sales — 2942097 rows saved.\n",
      "✅ Merged Pittsburgh with Sales — 721128 rows saved.\n",
      "✅ Merged Detroit with Sales — 447344 rows saved.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess sales once\n",
    "sales['GEOID'] = sales['GEOID'].astype(str).str.zfill(11)\n",
    "sales['City'] = sales['City'].str.upper()  # Fix: use 'City' (capitalized) instead of 'city'\n",
    "\n",
    "# Group by city for faster lookup\n",
    "sales_by_city = {city: df for city, df in sales.groupby('City')}\n",
    "\n",
    "# Columns to check for NaNs in income\n",
    "required_income_columns = [\n",
    "    'Median Income (Households)',\n",
    "    'Median Income (Families)',\n",
    "    'Median Income (Married Families)',\n",
    "    'Median Income (Nonfamily Households)'\n",
    "]\n",
    "\n",
    "# Function to drop junk columns\n",
    "def get_junk_columns(df):\n",
    "    return [col for col in df.columns if col.startswith('Unnamed') or col == 'parcel_id']\n",
    "\n",
    "# Store merged results\n",
    "acs_commerce_crime_merged_with_sales = {}\n",
    "\n",
    "for city in ['philadelphia', 'pittsburgh', 'detroit']:\n",
    "    base_df = merged_acs_commerce_crime_dfs.get(city)\n",
    "    city_sales = sales_by_city.get(city.upper())\n",
    "\n",
    "    if base_df is None:\n",
    "        print(f\"⚠️ No ACS+Commerce data found for {city}\")\n",
    "        continue\n",
    "    if city_sales is None:\n",
    "        print(f\"⚠️ No sales data found for {city}\")\n",
    "        continue\n",
    "\n",
    "    merged = pd.merge(base_df, city_sales, on='GEOID', how='inner')\n",
    "    merged.dropna(subset=required_income_columns, inplace=True)\n",
    "    merged.drop(columns=get_junk_columns(merged), inplace=True, errors='ignore')\n",
    "\n",
    "    acs_commerce_crime_merged_with_sales[city] = merged\n",
    "    merged.to_csv(f\"merged_{city}_acs_commerce_crime_sales.csv\", index=False)\n",
    "    print(f\"✅ Merged {city.capitalize()} with Sales — {len(merged)} rows saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "baf336d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Census Tract</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>Median Income (Households)</th>\n",
       "      <th>Median Income (Families)</th>\n",
       "      <th>Median Income (Married Families)</th>\n",
       "      <th>Median Income (Nonfamily Households)</th>\n",
       "      <th>Less than High School (%)</th>\n",
       "      <th>High School Graduate (%)</th>\n",
       "      <th>Some College or Associate Degree (%)</th>\n",
       "      <th>Bachelor's Degree or Higher (%)</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>White (%)</th>\n",
       "      <th>Black (%)</th>\n",
       "      <th>Asian (%)</th>\n",
       "      <th>Other (%)</th>\n",
       "      <th>Unemployment Rate (%)</th>\n",
       "      <th>Total Businesses</th>\n",
       "      <th>Nuisance Crime Rate</th>\n",
       "      <th>Other Crime Rate</th>\n",
       "      <th>Property Crime Rate</th>\n",
       "      <th>Violent Crime Rate</th>\n",
       "      <th>Total Crime Rate</th>\n",
       "      <th>City</th>\n",
       "      <th>Address</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Location</th>\n",
       "      <th>Sale Date</th>\n",
       "      <th>Sale Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Census Tract 10</td>\n",
       "      <td>42101001001</td>\n",
       "      <td>102329.0</td>\n",
       "      <td>171860.0</td>\n",
       "      <td>172874.0</td>\n",
       "      <td>77362.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.3</td>\n",
       "      <td>2256.0</td>\n",
       "      <td>93.218085</td>\n",
       "      <td>3.368794</td>\n",
       "      <td>2.792553</td>\n",
       "      <td>1.861702</td>\n",
       "      <td>1.3</td>\n",
       "      <td>21</td>\n",
       "      <td>16.59</td>\n",
       "      <td>45.12</td>\n",
       "      <td>98.59</td>\n",
       "      <td>36.76</td>\n",
       "      <td>197.06</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>821 CATHARINE ST</td>\n",
       "      <td>19147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-07 05:00:00+00:00</td>\n",
       "      <td>150000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Census Tract 10</td>\n",
       "      <td>42101001001</td>\n",
       "      <td>102329.0</td>\n",
       "      <td>171860.0</td>\n",
       "      <td>172874.0</td>\n",
       "      <td>77362.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.3</td>\n",
       "      <td>2256.0</td>\n",
       "      <td>93.218085</td>\n",
       "      <td>3.368794</td>\n",
       "      <td>2.792553</td>\n",
       "      <td>1.861702</td>\n",
       "      <td>1.3</td>\n",
       "      <td>21</td>\n",
       "      <td>16.59</td>\n",
       "      <td>45.12</td>\n",
       "      <td>98.59</td>\n",
       "      <td>36.76</td>\n",
       "      <td>197.06</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>727 SOUTH ST</td>\n",
       "      <td>19147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-11 05:00:00+00:00</td>\n",
       "      <td>190000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Census Tract 10</td>\n",
       "      <td>42101001001</td>\n",
       "      <td>102329.0</td>\n",
       "      <td>171860.0</td>\n",
       "      <td>172874.0</td>\n",
       "      <td>77362.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.3</td>\n",
       "      <td>2256.0</td>\n",
       "      <td>93.218085</td>\n",
       "      <td>3.368794</td>\n",
       "      <td>2.792553</td>\n",
       "      <td>1.861702</td>\n",
       "      <td>1.3</td>\n",
       "      <td>21</td>\n",
       "      <td>16.59</td>\n",
       "      <td>45.12</td>\n",
       "      <td>98.59</td>\n",
       "      <td>36.76</td>\n",
       "      <td>197.06</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>327 MANTON ST</td>\n",
       "      <td>19147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-11 05:00:00+00:00</td>\n",
       "      <td>40000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Census Tract 10</td>\n",
       "      <td>42101001001</td>\n",
       "      <td>102329.0</td>\n",
       "      <td>171860.0</td>\n",
       "      <td>172874.0</td>\n",
       "      <td>77362.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.3</td>\n",
       "      <td>2256.0</td>\n",
       "      <td>93.218085</td>\n",
       "      <td>3.368794</td>\n",
       "      <td>2.792553</td>\n",
       "      <td>1.861702</td>\n",
       "      <td>1.3</td>\n",
       "      <td>21</td>\n",
       "      <td>16.59</td>\n",
       "      <td>45.12</td>\n",
       "      <td>98.59</td>\n",
       "      <td>36.76</td>\n",
       "      <td>197.06</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>628 S 11TH ST UNIT B</td>\n",
       "      <td>19147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-11 05:00:00+00:00</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Census Tract 10</td>\n",
       "      <td>42101001001</td>\n",
       "      <td>102329.0</td>\n",
       "      <td>171860.0</td>\n",
       "      <td>172874.0</td>\n",
       "      <td>77362.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.3</td>\n",
       "      <td>2256.0</td>\n",
       "      <td>93.218085</td>\n",
       "      <td>3.368794</td>\n",
       "      <td>2.792553</td>\n",
       "      <td>1.861702</td>\n",
       "      <td>1.3</td>\n",
       "      <td>21</td>\n",
       "      <td>16.59</td>\n",
       "      <td>45.12</td>\n",
       "      <td>98.59</td>\n",
       "      <td>36.76</td>\n",
       "      <td>197.06</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>740 SAINT ALBANS ST</td>\n",
       "      <td>19147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-01-13 05:00:00+00:00</td>\n",
       "      <td>53000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959441</th>\n",
       "      <td>Census Tract 9802</td>\n",
       "      <td>42101980200</td>\n",
       "      <td>108717.0</td>\n",
       "      <td>120948.0</td>\n",
       "      <td>117801.0</td>\n",
       "      <td>68183.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.2</td>\n",
       "      <td>45.8</td>\n",
       "      <td>384.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97.916667</td>\n",
       "      <td>21.7</td>\n",
       "      <td>12</td>\n",
       "      <td>6.71</td>\n",
       "      <td>30.18</td>\n",
       "      <td>27.94</td>\n",
       "      <td>19.24</td>\n",
       "      <td>84.06</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>3404 ASHVILLE ST</td>\n",
       "      <td>19136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-11-17 05:00:00+00:00</td>\n",
       "      <td>142000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959442</th>\n",
       "      <td>Census Tract 9802</td>\n",
       "      <td>42101980200</td>\n",
       "      <td>108717.0</td>\n",
       "      <td>120948.0</td>\n",
       "      <td>117801.0</td>\n",
       "      <td>68183.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.2</td>\n",
       "      <td>45.8</td>\n",
       "      <td>384.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97.916667</td>\n",
       "      <td>21.7</td>\n",
       "      <td>12</td>\n",
       "      <td>6.71</td>\n",
       "      <td>30.18</td>\n",
       "      <td>27.94</td>\n",
       "      <td>19.24</td>\n",
       "      <td>84.06</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>4713 DECATUR ST</td>\n",
       "      <td>19136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-11-16 05:00:00+00:00</td>\n",
       "      <td>216500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959443</th>\n",
       "      <td>Census Tract 9802</td>\n",
       "      <td>42101980200</td>\n",
       "      <td>108717.0</td>\n",
       "      <td>120948.0</td>\n",
       "      <td>117801.0</td>\n",
       "      <td>68183.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.2</td>\n",
       "      <td>45.8</td>\n",
       "      <td>384.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97.916667</td>\n",
       "      <td>21.7</td>\n",
       "      <td>12</td>\n",
       "      <td>6.71</td>\n",
       "      <td>30.18</td>\n",
       "      <td>27.94</td>\n",
       "      <td>19.24</td>\n",
       "      <td>84.06</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>1844 EVARTS ST</td>\n",
       "      <td>19152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-11-17 05:00:00+00:00</td>\n",
       "      <td>189900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959444</th>\n",
       "      <td>Census Tract 9802</td>\n",
       "      <td>42101980200</td>\n",
       "      <td>108717.0</td>\n",
       "      <td>120948.0</td>\n",
       "      <td>117801.0</td>\n",
       "      <td>68183.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.2</td>\n",
       "      <td>45.8</td>\n",
       "      <td>384.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97.916667</td>\n",
       "      <td>21.7</td>\n",
       "      <td>12</td>\n",
       "      <td>6.71</td>\n",
       "      <td>30.18</td>\n",
       "      <td>27.94</td>\n",
       "      <td>19.24</td>\n",
       "      <td>84.06</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>4715 DECATUR ST</td>\n",
       "      <td>19136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-11-16 05:00:00+00:00</td>\n",
       "      <td>83500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959445</th>\n",
       "      <td>Census Tract 9802</td>\n",
       "      <td>42101980200</td>\n",
       "      <td>108717.0</td>\n",
       "      <td>120948.0</td>\n",
       "      <td>117801.0</td>\n",
       "      <td>68183.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.2</td>\n",
       "      <td>45.8</td>\n",
       "      <td>384.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97.916667</td>\n",
       "      <td>21.7</td>\n",
       "      <td>12</td>\n",
       "      <td>6.71</td>\n",
       "      <td>30.18</td>\n",
       "      <td>27.94</td>\n",
       "      <td>19.24</td>\n",
       "      <td>84.06</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>4318 BLAKISTON ST</td>\n",
       "      <td>19136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-11-18 05:00:00+00:00</td>\n",
       "      <td>260000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2942097 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Census Tract        GEOID  Median Income (Households)  \\\n",
       "0          Census Tract 10  42101001001                    102329.0   \n",
       "1          Census Tract 10  42101001001                    102329.0   \n",
       "2          Census Tract 10  42101001001                    102329.0   \n",
       "3          Census Tract 10  42101001001                    102329.0   \n",
       "4          Census Tract 10  42101001001                    102329.0   \n",
       "...                    ...          ...                         ...   \n",
       "2959441  Census Tract 9802  42101980200                    108717.0   \n",
       "2959442  Census Tract 9802  42101980200                    108717.0   \n",
       "2959443  Census Tract 9802  42101980200                    108717.0   \n",
       "2959444  Census Tract 9802  42101980200                    108717.0   \n",
       "2959445  Census Tract 9802  42101980200                    108717.0   \n",
       "\n",
       "         Median Income (Families)  Median Income (Married Families)  \\\n",
       "0                        171860.0                          172874.0   \n",
       "1                        171860.0                          172874.0   \n",
       "2                        171860.0                          172874.0   \n",
       "3                        171860.0                          172874.0   \n",
       "4                        171860.0                          172874.0   \n",
       "...                           ...                               ...   \n",
       "2959441                  120948.0                          117801.0   \n",
       "2959442                  120948.0                          117801.0   \n",
       "2959443                  120948.0                          117801.0   \n",
       "2959444                  120948.0                          117801.0   \n",
       "2959445                  120948.0                          117801.0   \n",
       "\n",
       "         Median Income (Nonfamily Households) Less than High School (%)  \\\n",
       "0                                     77362.0                       0.0   \n",
       "1                                     77362.0                       0.0   \n",
       "2                                     77362.0                       0.0   \n",
       "3                                     77362.0                       0.0   \n",
       "4                                     77362.0                       0.0   \n",
       "...                                       ...                       ...   \n",
       "2959441                               68183.0                       0.0   \n",
       "2959442                               68183.0                       0.0   \n",
       "2959443                               68183.0                       0.0   \n",
       "2959444                               68183.0                       0.0   \n",
       "2959445                               68183.0                       0.0   \n",
       "\n",
       "        High School Graduate (%) Some College or Associate Degree (%)  \\\n",
       "0                           61.7                                  0.0   \n",
       "1                           61.7                                  0.0   \n",
       "2                           61.7                                  0.0   \n",
       "3                           61.7                                  0.0   \n",
       "4                           61.7                                  0.0   \n",
       "...                          ...                                  ...   \n",
       "2959441                      0.0                                 54.2   \n",
       "2959442                      0.0                                 54.2   \n",
       "2959443                      0.0                                 54.2   \n",
       "2959444                      0.0                                 54.2   \n",
       "2959445                      0.0                                 54.2   \n",
       "\n",
       "        Bachelor's Degree or Higher (%)  Total Population   White (%)  \\\n",
       "0                                  38.3            2256.0   93.218085   \n",
       "1                                  38.3            2256.0   93.218085   \n",
       "2                                  38.3            2256.0   93.218085   \n",
       "3                                  38.3            2256.0   93.218085   \n",
       "4                                  38.3            2256.0   93.218085   \n",
       "...                                 ...               ...         ...   \n",
       "2959441                            45.8             384.0  100.000000   \n",
       "2959442                            45.8             384.0  100.000000   \n",
       "2959443                            45.8             384.0  100.000000   \n",
       "2959444                            45.8             384.0  100.000000   \n",
       "2959445                            45.8             384.0  100.000000   \n",
       "\n",
       "         Black (%)  Asian (%)  Other (%)  Unemployment Rate (%)  \\\n",
       "0         3.368794   2.792553   1.861702                    1.3   \n",
       "1         3.368794   2.792553   1.861702                    1.3   \n",
       "2         3.368794   2.792553   1.861702                    1.3   \n",
       "3         3.368794   2.792553   1.861702                    1.3   \n",
       "4         3.368794   2.792553   1.861702                    1.3   \n",
       "...            ...        ...        ...                    ...   \n",
       "2959441   0.000000   0.000000  97.916667                   21.7   \n",
       "2959442   0.000000   0.000000  97.916667                   21.7   \n",
       "2959443   0.000000   0.000000  97.916667                   21.7   \n",
       "2959444   0.000000   0.000000  97.916667                   21.7   \n",
       "2959445   0.000000   0.000000  97.916667                   21.7   \n",
       "\n",
       "         Total Businesses  Nuisance Crime Rate  Other Crime Rate  \\\n",
       "0                      21                16.59             45.12   \n",
       "1                      21                16.59             45.12   \n",
       "2                      21                16.59             45.12   \n",
       "3                      21                16.59             45.12   \n",
       "4                      21                16.59             45.12   \n",
       "...                   ...                  ...               ...   \n",
       "2959441                12                 6.71             30.18   \n",
       "2959442                12                 6.71             30.18   \n",
       "2959443                12                 6.71             30.18   \n",
       "2959444                12                 6.71             30.18   \n",
       "2959445                12                 6.71             30.18   \n",
       "\n",
       "         Property Crime Rate  Violent Crime Rate  Total Crime Rate  \\\n",
       "0                      98.59               36.76            197.06   \n",
       "1                      98.59               36.76            197.06   \n",
       "2                      98.59               36.76            197.06   \n",
       "3                      98.59               36.76            197.06   \n",
       "4                      98.59               36.76            197.06   \n",
       "...                      ...                 ...               ...   \n",
       "2959441                27.94               19.24             84.06   \n",
       "2959442                27.94               19.24             84.06   \n",
       "2959443                27.94               19.24             84.06   \n",
       "2959444                27.94               19.24             84.06   \n",
       "2959445                27.94               19.24             84.06   \n",
       "\n",
       "                 City               Address  Zip Code Location  \\\n",
       "0        PHILADELPHIA      821 CATHARINE ST     19147      NaN   \n",
       "1        PHILADELPHIA          727 SOUTH ST     19147      NaN   \n",
       "2        PHILADELPHIA         327 MANTON ST     19147      NaN   \n",
       "3        PHILADELPHIA  628 S 11TH ST UNIT B     19147      NaN   \n",
       "4        PHILADELPHIA   740 SAINT ALBANS ST     19147      NaN   \n",
       "...               ...                   ...       ...      ...   \n",
       "2959441  PHILADELPHIA      3404 ASHVILLE ST     19136      NaN   \n",
       "2959442  PHILADELPHIA       4713 DECATUR ST     19136      NaN   \n",
       "2959443  PHILADELPHIA        1844 EVARTS ST     19152      NaN   \n",
       "2959444  PHILADELPHIA       4715 DECATUR ST     19136      NaN   \n",
       "2959445  PHILADELPHIA     4318 BLAKISTON ST     19136      NaN   \n",
       "\n",
       "                         Sale Date  Sale Price  \n",
       "0        2000-01-07 05:00:00+00:00    150000.0  \n",
       "1        2000-01-11 05:00:00+00:00    190000.0  \n",
       "2        2000-01-11 05:00:00+00:00     40000.0  \n",
       "3        2000-01-11 05:00:00+00:00     70000.0  \n",
       "4        2000-01-13 05:00:00+00:00     53000.0  \n",
       "...                            ...         ...  \n",
       "2959441  2006-11-17 05:00:00+00:00    142000.0  \n",
       "2959442  2006-11-16 05:00:00+00:00    216500.0  \n",
       "2959443  2006-11-17 05:00:00+00:00    189900.0  \n",
       "2959444  2006-11-16 05:00:00+00:00     83500.0  \n",
       "2959445  2006-11-18 05:00:00+00:00    260000.0  \n",
       "\n",
       "[2942097 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_columns', None):\n",
    "    display(acs_commerce_crime_merged_with_sales['philadelphia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "23e0d96c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 --- PHILADELPHIA ---\n",
      "📌 DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2942097 entries, 0 to 2959445\n",
      "Data columns (total 28 columns):\n",
      " #   Column                                Dtype  \n",
      "---  ------                                -----  \n",
      " 0   Census Tract                          object \n",
      " 1   GEOID                                 object \n",
      " 2   Median Income (Households)            float64\n",
      " 3   Median Income (Families)              float64\n",
      " 4   Median Income (Married Families)      float64\n",
      " 5   Median Income (Nonfamily Households)  float64\n",
      " 6   Less than High School (%)             object \n",
      " 7   High School Graduate (%)              object \n",
      " 8   Some College or Associate Degree (%)  object \n",
      " 9   Bachelor's Degree or Higher (%)       object \n",
      " 10  Total Population                      float64\n",
      " 11  White (%)                             float64\n",
      " 12  Black (%)                             float64\n",
      " 13  Asian (%)                             float64\n",
      " 14  Other (%)                             float64\n",
      " 15  Unemployment Rate (%)                 float64\n",
      " 16  Total Businesses                      int64  \n",
      " 17  Nuisance Crime Rate                   float64\n",
      " 18  Other Crime Rate                      float64\n",
      " 19  Property Crime Rate                   float64\n",
      " 20  Violent Crime Rate                    float64\n",
      " 21  Total Crime Rate                      float64\n",
      " 22  City                                  object \n",
      " 23  Address                               object \n",
      " 24  Zip Code                              int64  \n",
      " 25  Location                              object \n",
      " 26  Sale Date                             object \n",
      " 27  Sale Price                            float64\n",
      "dtypes: float64(16), int64(2), object(10)\n",
      "memory usage: 650.9+ MB\n",
      "None\n",
      "\n",
      "📈 Descriptive Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Census Tract</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>Median Income (Households)</th>\n",
       "      <th>Median Income (Families)</th>\n",
       "      <th>Median Income (Married Families)</th>\n",
       "      <th>Median Income (Nonfamily Households)</th>\n",
       "      <th>Less than High School (%)</th>\n",
       "      <th>High School Graduate (%)</th>\n",
       "      <th>Some College or Associate Degree (%)</th>\n",
       "      <th>Bachelor's Degree or Higher (%)</th>\n",
       "      <th>...</th>\n",
       "      <th>Other Crime Rate</th>\n",
       "      <th>Property Crime Rate</th>\n",
       "      <th>Violent Crime Rate</th>\n",
       "      <th>Total Crime Rate</th>\n",
       "      <th>City</th>\n",
       "      <th>Address</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Location</th>\n",
       "      <th>Sale Date</th>\n",
       "      <th>Sale Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2942097</td>\n",
       "      <td>2942097</td>\n",
       "      <td>2.942097e+06</td>\n",
       "      <td>2.942097e+06</td>\n",
       "      <td>2.942097e+06</td>\n",
       "      <td>2.942097e+06</td>\n",
       "      <td>2942097</td>\n",
       "      <td>2942097</td>\n",
       "      <td>2942097</td>\n",
       "      <td>2942097</td>\n",
       "      <td>...</td>\n",
       "      <td>2.942097e+06</td>\n",
       "      <td>2.942097e+06</td>\n",
       "      <td>2.942097e+06</td>\n",
       "      <td>2.942097e+06</td>\n",
       "      <td>2942097</td>\n",
       "      <td>2942097</td>\n",
       "      <td>2.942097e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>2942097</td>\n",
       "      <td>2.942097e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>297</td>\n",
       "      <td>348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>251</td>\n",
       "      <td>277</td>\n",
       "      <td>192</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>167354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1848</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Census Tract 289</td>\n",
       "      <td>42101038200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>9228-38 BLUE GRASS RD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-07-14 04:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>47524</td>\n",
       "      <td>25954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>474497</td>\n",
       "      <td>119752</td>\n",
       "      <td>50455</td>\n",
       "      <td>676963</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2942097</td>\n",
       "      <td>240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3658</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.625556e+04</td>\n",
       "      <td>5.985524e+04</td>\n",
       "      <td>7.719474e+04</td>\n",
       "      <td>3.308358e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.057016e+02</td>\n",
       "      <td>1.514485e+02</td>\n",
       "      <td>1.307200e+02</td>\n",
       "      <td>4.310615e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.913303e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.617071e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.068106e+04</td>\n",
       "      <td>3.268648e+04</td>\n",
       "      <td>3.258420e+04</td>\n",
       "      <td>1.647886e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.409406e+01</td>\n",
       "      <td>9.501761e+01</td>\n",
       "      <td>7.471970e+01</td>\n",
       "      <td>2.845752e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.313761e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.138874e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.335400e+04</td>\n",
       "      <td>1.615600e+04</td>\n",
       "      <td>2.551900e+04</td>\n",
       "      <td>9.334000e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.470000e+00</td>\n",
       "      <td>1.653000e+01</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>3.335000e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.910200e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000100e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.127000e+04</td>\n",
       "      <td>3.784500e+04</td>\n",
       "      <td>5.546800e+04</td>\n",
       "      <td>2.197500e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.688000e+01</td>\n",
       "      <td>9.612000e+01</td>\n",
       "      <td>7.124000e+01</td>\n",
       "      <td>2.353500e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.912400e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.313500e+04</td>\n",
       "      <td>4.943400e+04</td>\n",
       "      <td>6.798600e+04</td>\n",
       "      <td>2.974200e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.612000e+01</td>\n",
       "      <td>1.355300e+02</td>\n",
       "      <td>1.197600e+02</td>\n",
       "      <td>3.630000e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.913400e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.100000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.618400e+04</td>\n",
       "      <td>7.175900e+04</td>\n",
       "      <td>9.117500e+04</td>\n",
       "      <td>4.085400e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.281200e+02</td>\n",
       "      <td>1.795300e+02</td>\n",
       "      <td>1.792400e+02</td>\n",
       "      <td>5.546500e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.914500e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.360000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.302220e+05</td>\n",
       "      <td>1.844670e+05</td>\n",
       "      <td>1.998280e+05</td>\n",
       "      <td>1.136640e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6.511700e+02</td>\n",
       "      <td>1.034670e+03</td>\n",
       "      <td>3.734700e+02</td>\n",
       "      <td>1.854000e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.915400e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.835000e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Census Tract        GEOID  Median Income (Households)  \\\n",
       "count            2942097      2942097                2.942097e+06   \n",
       "unique               297          348                         NaN   \n",
       "top     Census Tract 289  42101038200                         NaN   \n",
       "freq               47524        25954                         NaN   \n",
       "mean                 NaN          NaN                4.625556e+04   \n",
       "std                  NaN          NaN                2.068106e+04   \n",
       "min                  NaN          NaN                1.335400e+04   \n",
       "25%                  NaN          NaN                3.127000e+04   \n",
       "50%                  NaN          NaN                4.313500e+04   \n",
       "75%                  NaN          NaN                5.618400e+04   \n",
       "max                  NaN          NaN                1.302220e+05   \n",
       "\n",
       "        Median Income (Families)  Median Income (Married Families)  \\\n",
       "count               2.942097e+06                      2.942097e+06   \n",
       "unique                       NaN                               NaN   \n",
       "top                          NaN                               NaN   \n",
       "freq                         NaN                               NaN   \n",
       "mean                5.985524e+04                      7.719474e+04   \n",
       "std                 3.268648e+04                      3.258420e+04   \n",
       "min                 1.615600e+04                      2.551900e+04   \n",
       "25%                 3.784500e+04                      5.546800e+04   \n",
       "50%                 4.943400e+04                      6.798600e+04   \n",
       "75%                 7.175900e+04                      9.117500e+04   \n",
       "max                 1.844670e+05                      1.998280e+05   \n",
       "\n",
       "        Median Income (Nonfamily Households) Less than High School (%)  \\\n",
       "count                           2.942097e+06                   2942097   \n",
       "unique                                   NaN                       200   \n",
       "top                                      NaN                       0.0   \n",
       "freq                                     NaN                    474497   \n",
       "mean                            3.308358e+04                       NaN   \n",
       "std                             1.647886e+04                       NaN   \n",
       "min                             9.334000e+03                       NaN   \n",
       "25%                             2.197500e+04                       NaN   \n",
       "50%                             2.974200e+04                       NaN   \n",
       "75%                             4.085400e+04                       NaN   \n",
       "max                             1.136640e+05                       NaN   \n",
       "\n",
       "       High School Graduate (%) Some College or Associate Degree (%)  \\\n",
       "count                   2942097                              2942097   \n",
       "unique                      251                                  277   \n",
       "top                         0.0                                  0.0   \n",
       "freq                     119752                                50455   \n",
       "mean                        NaN                                  NaN   \n",
       "std                         NaN                                  NaN   \n",
       "min                         NaN                                  NaN   \n",
       "25%                         NaN                                  NaN   \n",
       "50%                         NaN                                  NaN   \n",
       "75%                         NaN                                  NaN   \n",
       "max                         NaN                                  NaN   \n",
       "\n",
       "       Bachelor's Degree or Higher (%)  ...  Other Crime Rate  \\\n",
       "count                          2942097  ...      2.942097e+06   \n",
       "unique                             192  ...               NaN   \n",
       "top                                0.0  ...               NaN   \n",
       "freq                            676963  ...               NaN   \n",
       "mean                               NaN  ...      1.057016e+02   \n",
       "std                                NaN  ...      9.409406e+01   \n",
       "min                                NaN  ...      4.470000e+00   \n",
       "25%                                NaN  ...      4.688000e+01   \n",
       "50%                                NaN  ...      7.612000e+01   \n",
       "75%                                NaN  ...      1.281200e+02   \n",
       "max                                NaN  ...      6.511700e+02   \n",
       "\n",
       "        Property Crime Rate  Violent Crime Rate  Total Crime Rate  \\\n",
       "count          2.942097e+06        2.942097e+06      2.942097e+06   \n",
       "unique                  NaN                 NaN               NaN   \n",
       "top                     NaN                 NaN               NaN   \n",
       "freq                    NaN                 NaN               NaN   \n",
       "mean           1.514485e+02        1.307200e+02      4.310615e+02   \n",
       "std            9.501761e+01        7.471970e+01      2.845752e+02   \n",
       "min            1.653000e+01        7.000000e+00      3.335000e+01   \n",
       "25%            9.612000e+01        7.124000e+01      2.353500e+02   \n",
       "50%            1.355300e+02        1.197600e+02      3.630000e+02   \n",
       "75%            1.795300e+02        1.792400e+02      5.546500e+02   \n",
       "max            1.034670e+03        3.734700e+02      1.854000e+03   \n",
       "\n",
       "                City                Address      Zip Code  Location  \\\n",
       "count        2942097                2942097  2.942097e+06         0   \n",
       "unique             1                 167354           NaN         0   \n",
       "top     PHILADELPHIA  9228-38 BLUE GRASS RD           NaN       NaN   \n",
       "freq         2942097                    240           NaN       NaN   \n",
       "mean             NaN                    NaN  1.913303e+04       NaN   \n",
       "std              NaN                    NaN  1.313761e+01       NaN   \n",
       "min              NaN                    NaN  1.910200e+04       NaN   \n",
       "25%              NaN                    NaN  1.912400e+04       NaN   \n",
       "50%              NaN                    NaN  1.913400e+04       NaN   \n",
       "75%              NaN                    NaN  1.914500e+04       NaN   \n",
       "max              NaN                    NaN  1.915400e+04       NaN   \n",
       "\n",
       "                        Sale Date    Sale Price  \n",
       "count                     2942097  2.942097e+06  \n",
       "unique                       1848           NaN  \n",
       "top     2004-07-14 04:00:00+00:00           NaN  \n",
       "freq                         3658           NaN  \n",
       "mean                          NaN  1.617071e+05  \n",
       "std                           NaN  1.138874e+06  \n",
       "min                           NaN  1.000100e+04  \n",
       "25%                           NaN  4.000000e+04  \n",
       "50%                           NaN  7.100000e+04  \n",
       "75%                           NaN  1.360000e+05  \n",
       "max                           NaN  1.835000e+08  \n",
       "\n",
       "[11 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 --- PITTSBURGH ---\n",
      "📌 DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 721128 entries, 0 to 723255\n",
      "Data columns (total 30 columns):\n",
      " #   Column                                Non-Null Count   Dtype  \n",
      "---  ------                                --------------   -----  \n",
      " 0   Census Tract                          721128 non-null  object \n",
      " 1   GEOID                                 721128 non-null  object \n",
      " 2   Median Income (Households)            721128 non-null  float64\n",
      " 3   Median Income (Families)              721128 non-null  float64\n",
      " 4   Median Income (Married Families)      721128 non-null  float64\n",
      " 5   Median Income (Nonfamily Households)  721128 non-null  float64\n",
      " 6   Less than High School (%)             721128 non-null  object \n",
      " 7   High School Graduate (%)              721128 non-null  object \n",
      " 8   Some College or Associate Degree (%)  721128 non-null  object \n",
      " 9   Bachelor's Degree or Higher (%)       721128 non-null  object \n",
      " 10  Total Population                      721128 non-null  float64\n",
      " 11  White (%)                             721128 non-null  float64\n",
      " 12  Black (%)                             721128 non-null  float64\n",
      " 13  Asian (%)                             721128 non-null  float64\n",
      " 14  Other (%)                             721128 non-null  float64\n",
      " 15  Unemployment Rate (%)                 721128 non-null  float64\n",
      " 16  Total Businesses                      721128 non-null  int64  \n",
      " 17  INCIDENTTRACT                         721128 non-null  float64\n",
      " 18  Nuisance Crime Rate                   721128 non-null  float64\n",
      " 19  Other Crime Rate                      721128 non-null  float64\n",
      " 20  Property Crime Rate                   721128 non-null  float64\n",
      " 21  Violent Crime Rate                    721128 non-null  float64\n",
      " 22  Total Crime Rate                      721128 non-null  float64\n",
      " 23  tract                                 721128 non-null  float64\n",
      " 24  City                                  721128 non-null  object \n",
      " 25  Address                               721128 non-null  object \n",
      " 26  Zip Code                              721128 non-null  int64  \n",
      " 27  Location                              0 non-null       object \n",
      " 28  Sale Date                             721128 non-null  object \n",
      " 29  Sale Price                            721128 non-null  float64\n",
      "dtypes: float64(18), int64(2), object(10)\n",
      "memory usage: 170.6+ MB\n",
      "None\n",
      "\n",
      "📈 Descriptive Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Census Tract</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>Median Income (Households)</th>\n",
       "      <th>Median Income (Families)</th>\n",
       "      <th>Median Income (Married Families)</th>\n",
       "      <th>Median Income (Nonfamily Households)</th>\n",
       "      <th>Less than High School (%)</th>\n",
       "      <th>High School Graduate (%)</th>\n",
       "      <th>Some College or Associate Degree (%)</th>\n",
       "      <th>Bachelor's Degree or Higher (%)</th>\n",
       "      <th>...</th>\n",
       "      <th>Property Crime Rate</th>\n",
       "      <th>Violent Crime Rate</th>\n",
       "      <th>Total Crime Rate</th>\n",
       "      <th>tract</th>\n",
       "      <th>City</th>\n",
       "      <th>Address</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Location</th>\n",
       "      <th>Sale Date</th>\n",
       "      <th>Sale Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>721128</td>\n",
       "      <td>721128</td>\n",
       "      <td>721128.000000</td>\n",
       "      <td>721128.000000</td>\n",
       "      <td>721128.000000</td>\n",
       "      <td>721128.000000</td>\n",
       "      <td>721128</td>\n",
       "      <td>721128</td>\n",
       "      <td>721128</td>\n",
       "      <td>721128</td>\n",
       "      <td>...</td>\n",
       "      <td>721128.000000</td>\n",
       "      <td>721128.000000</td>\n",
       "      <td>721128.000000</td>\n",
       "      <td>721128.000000</td>\n",
       "      <td>721128</td>\n",
       "      <td>721128</td>\n",
       "      <td>721128.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>721128</td>\n",
       "      <td>7.211280e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>75974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4282</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Census Tract 3207</td>\n",
       "      <td>42003320700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PITTSBURGH</td>\n",
       "      <td>0 LINCOLN AVE, PITTSBURGH, PA 15206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>21495</td>\n",
       "      <td>21495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>175340</td>\n",
       "      <td>35010</td>\n",
       "      <td>33258</td>\n",
       "      <td>98419</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>721128</td>\n",
       "      <td>496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51125.300403</td>\n",
       "      <td>71005.716719</td>\n",
       "      <td>88669.651137</td>\n",
       "      <td>36389.619972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19.080124</td>\n",
       "      <td>18.020011</td>\n",
       "      <td>122.501882</td>\n",
       "      <td>2027.781620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15213.888068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.901740e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22972.616716</td>\n",
       "      <td>33645.460866</td>\n",
       "      <td>30558.469881</td>\n",
       "      <td>13268.137620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19.499951</td>\n",
       "      <td>19.032623</td>\n",
       "      <td>135.755840</td>\n",
       "      <td>1369.658852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.752289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.336606e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13111.000000</td>\n",
       "      <td>19408.000000</td>\n",
       "      <td>32366.000000</td>\n",
       "      <td>10849.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15201.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000100e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35669.000000</td>\n",
       "      <td>48835.000000</td>\n",
       "      <td>70341.000000</td>\n",
       "      <td>25657.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.620000</td>\n",
       "      <td>6.120000</td>\n",
       "      <td>45.120000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15206.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.700000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49929.000000</td>\n",
       "      <td>65646.000000</td>\n",
       "      <td>81843.000000</td>\n",
       "      <td>36191.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>1706.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15212.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.479000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59111.000000</td>\n",
       "      <td>80135.000000</td>\n",
       "      <td>101878.000000</td>\n",
       "      <td>43703.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>152.620000</td>\n",
       "      <td>2708.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15219.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.700000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144624.000000</td>\n",
       "      <td>182808.000000</td>\n",
       "      <td>186851.000000</td>\n",
       "      <td>85923.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>144.800000</td>\n",
       "      <td>114.700000</td>\n",
       "      <td>1076.700000</td>\n",
       "      <td>5630.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15235.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.487529e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Census Tract        GEOID  Median Income (Households)  \\\n",
       "count              721128       721128               721128.000000   \n",
       "unique                 92           92                         NaN   \n",
       "top     Census Tract 3207  42003320700                         NaN   \n",
       "freq                21495        21495                         NaN   \n",
       "mean                  NaN          NaN                51125.300403   \n",
       "std                   NaN          NaN                22972.616716   \n",
       "min                   NaN          NaN                13111.000000   \n",
       "25%                   NaN          NaN                35669.000000   \n",
       "50%                   NaN          NaN                49929.000000   \n",
       "75%                   NaN          NaN                59111.000000   \n",
       "max                   NaN          NaN               144624.000000   \n",
       "\n",
       "        Median Income (Families)  Median Income (Married Families)  \\\n",
       "count              721128.000000                     721128.000000   \n",
       "unique                       NaN                               NaN   \n",
       "top                          NaN                               NaN   \n",
       "freq                         NaN                               NaN   \n",
       "mean                71005.716719                      88669.651137   \n",
       "std                 33645.460866                      30558.469881   \n",
       "min                 19408.000000                      32366.000000   \n",
       "25%                 48835.000000                      70341.000000   \n",
       "50%                 65646.000000                      81843.000000   \n",
       "75%                 80135.000000                     101878.000000   \n",
       "max                182808.000000                     186851.000000   \n",
       "\n",
       "        Median Income (Nonfamily Households) Less than High School (%)  \\\n",
       "count                          721128.000000                    721128   \n",
       "unique                                   NaN                        61   \n",
       "top                                      NaN                       0.0   \n",
       "freq                                     NaN                    175340   \n",
       "mean                            36389.619972                       NaN   \n",
       "std                             13268.137620                       NaN   \n",
       "min                             10849.000000                       NaN   \n",
       "25%                             25657.000000                       NaN   \n",
       "50%                             36191.000000                       NaN   \n",
       "75%                             43703.000000                       NaN   \n",
       "max                             85923.000000                       NaN   \n",
       "\n",
       "       High School Graduate (%) Some College or Associate Degree (%)  \\\n",
       "count                    721128                               721128   \n",
       "unique                       85                                   86   \n",
       "top                        35.7                                  0.0   \n",
       "freq                      35010                                33258   \n",
       "mean                        NaN                                  NaN   \n",
       "std                         NaN                                  NaN   \n",
       "min                         NaN                                  NaN   \n",
       "25%                         NaN                                  NaN   \n",
       "50%                         NaN                                  NaN   \n",
       "75%                         NaN                                  NaN   \n",
       "max                         NaN                                  NaN   \n",
       "\n",
       "       Bachelor's Degree or Higher (%)  ...  Property Crime Rate  \\\n",
       "count                           721128  ...        721128.000000   \n",
       "unique                              78  ...                  NaN   \n",
       "top                                0.0  ...                  NaN   \n",
       "freq                             98419  ...                  NaN   \n",
       "mean                               NaN  ...            19.080124   \n",
       "std                                NaN  ...            19.499951   \n",
       "min                                NaN  ...             0.000000   \n",
       "25%                                NaN  ...             8.620000   \n",
       "50%                                NaN  ...            13.750000   \n",
       "75%                                NaN  ...            23.250000   \n",
       "max                                NaN  ...           144.800000   \n",
       "\n",
       "        Violent Crime Rate  Total Crime Rate          tract        City  \\\n",
       "count        721128.000000     721128.000000  721128.000000      721128   \n",
       "unique                 NaN               NaN            NaN           1   \n",
       "top                    NaN               NaN            NaN  PITTSBURGH   \n",
       "freq                   NaN               NaN            NaN      721128   \n",
       "mean             18.020011        122.501882    2027.781620         NaN   \n",
       "std              19.032623        135.755840    1369.658852         NaN   \n",
       "min               0.000000          2.000000     201.000000         NaN   \n",
       "25%               6.120000         45.120000    1018.000000         NaN   \n",
       "50%              11.110000         85.000000    1706.000000         NaN   \n",
       "75%              23.000000        152.620000    2708.000000         NaN   \n",
       "max             114.700000       1076.700000    5630.000000         NaN   \n",
       "\n",
       "                                    Address       Zip Code  Location  \\\n",
       "count                                721128  721128.000000         0   \n",
       "unique                                75974            NaN         0   \n",
       "top     0 LINCOLN AVE, PITTSBURGH, PA 15206            NaN       NaN   \n",
       "freq                                    496            NaN       NaN   \n",
       "mean                                    NaN   15213.888068       NaN   \n",
       "std                                     NaN       8.752289       NaN   \n",
       "min                                     NaN   15201.000000       NaN   \n",
       "25%                                     NaN   15206.000000       NaN   \n",
       "50%                                     NaN   15212.000000       NaN   \n",
       "75%                                     NaN   15219.000000       NaN   \n",
       "max                                     NaN   15235.000000       NaN   \n",
       "\n",
       "         Sale Date    Sale Price  \n",
       "count       721128  7.211280e+05  \n",
       "unique        4282           NaN  \n",
       "top     2016-04-29           NaN  \n",
       "freq          1902           NaN  \n",
       "mean           NaN  2.901740e+05  \n",
       "std            NaN  1.336606e+06  \n",
       "min            NaN  1.000100e+04  \n",
       "25%            NaN  6.700000e+04  \n",
       "50%            NaN  1.479000e+05  \n",
       "75%            NaN  2.700000e+05  \n",
       "max            NaN  1.487529e+08  \n",
       "\n",
       "[11 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 --- DETROIT ---\n",
      "📌 DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 447344 entries, 0 to 448102\n",
      "Data columns (total 28 columns):\n",
      " #   Column                                Non-Null Count   Dtype  \n",
      "---  ------                                --------------   -----  \n",
      " 0   Census Tract                          447344 non-null  object \n",
      " 1   GEOID                                 447344 non-null  object \n",
      " 2   Median Income (Households)            447344 non-null  float64\n",
      " 3   Median Income (Families)              447344 non-null  float64\n",
      " 4   Median Income (Married Families)      447344 non-null  float64\n",
      " 5   Median Income (Nonfamily Households)  447344 non-null  float64\n",
      " 6   Less than High School (%)             447344 non-null  object \n",
      " 7   High School Graduate (%)              447344 non-null  object \n",
      " 8   Some College or Associate Degree (%)  447344 non-null  object \n",
      " 9   Bachelor's Degree or Higher (%)       447344 non-null  object \n",
      " 10  Total Population                      447344 non-null  float64\n",
      " 11  White (%)                             447344 non-null  float64\n",
      " 12  Black (%)                             447344 non-null  float64\n",
      " 13  Asian (%)                             447344 non-null  float64\n",
      " 14  Other (%)                             447344 non-null  float64\n",
      " 15  Unemployment Rate (%)                 447344 non-null  float64\n",
      " 16  Total Businesses                      447344 non-null  int64  \n",
      " 17  Nuisance Crime Rate                   447344 non-null  float64\n",
      " 18  Other Crime Rate                      447344 non-null  float64\n",
      " 19  Property Crime Rate                   447344 non-null  float64\n",
      " 20  Violent Crime Rate                    447344 non-null  float64\n",
      " 21  Total Crime Rate                      447344 non-null  float64\n",
      " 22  City                                  447344 non-null  object \n",
      " 23  Address                               441365 non-null  object \n",
      " 24  Zip Code                              447344 non-null  int64  \n",
      " 25  Location                              447344 non-null  object \n",
      " 26  Sale Date                             447344 non-null  object \n",
      " 27  Sale Price                            447344 non-null  float64\n",
      "dtypes: float64(16), int64(2), object(10)\n",
      "memory usage: 99.0+ MB\n",
      "None\n",
      "\n",
      "📈 Descriptive Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Census Tract</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>Median Income (Households)</th>\n",
       "      <th>Median Income (Families)</th>\n",
       "      <th>Median Income (Married Families)</th>\n",
       "      <th>Median Income (Nonfamily Households)</th>\n",
       "      <th>Less than High School (%)</th>\n",
       "      <th>High School Graduate (%)</th>\n",
       "      <th>Some College or Associate Degree (%)</th>\n",
       "      <th>Bachelor's Degree or Higher (%)</th>\n",
       "      <th>...</th>\n",
       "      <th>Other Crime Rate</th>\n",
       "      <th>Property Crime Rate</th>\n",
       "      <th>Violent Crime Rate</th>\n",
       "      <th>Total Crime Rate</th>\n",
       "      <th>City</th>\n",
       "      <th>Address</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Location</th>\n",
       "      <th>Sale Date</th>\n",
       "      <th>Sale Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>447344</td>\n",
       "      <td>447344</td>\n",
       "      <td>447344.000000</td>\n",
       "      <td>447344.000000</td>\n",
       "      <td>447344.000000</td>\n",
       "      <td>447344.000000</td>\n",
       "      <td>447344</td>\n",
       "      <td>447344</td>\n",
       "      <td>447344</td>\n",
       "      <td>447344</td>\n",
       "      <td>...</td>\n",
       "      <td>447344.000000</td>\n",
       "      <td>447344.000000</td>\n",
       "      <td>447344.000000</td>\n",
       "      <td>447344.000000</td>\n",
       "      <td>447344</td>\n",
       "      <td>441365</td>\n",
       "      <td>447344.000000</td>\n",
       "      <td>447344</td>\n",
       "      <td>447344</td>\n",
       "      <td>4.473440e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>261</td>\n",
       "      <td>261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>213</td>\n",
       "      <td>215</td>\n",
       "      <td>207</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>211046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>212627</td>\n",
       "      <td>5105</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Census Tract 5458</td>\n",
       "      <td>26163545800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DETROIT</td>\n",
       "      <td>244 MADISON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (-83.0557768411244 42.3237930055647)</td>\n",
       "      <td>1/23/2014 12:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4721</td>\n",
       "      <td>4721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19231</td>\n",
       "      <td>7098</td>\n",
       "      <td>13336</td>\n",
       "      <td>247788</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>447344</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>4628</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32262.784300</td>\n",
       "      <td>38602.177946</td>\n",
       "      <td>59586.092484</td>\n",
       "      <td>23700.777858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.896529</td>\n",
       "      <td>3.253322</td>\n",
       "      <td>2.394017</td>\n",
       "      <td>6.684153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48219.157557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.886125e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10739.229403</td>\n",
       "      <td>13419.680392</td>\n",
       "      <td>17912.912338</td>\n",
       "      <td>8007.141849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392914</td>\n",
       "      <td>1.247353</td>\n",
       "      <td>0.946173</td>\n",
       "      <td>2.401048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.821304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.054315e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11121.000000</td>\n",
       "      <td>14449.000000</td>\n",
       "      <td>15182.000000</td>\n",
       "      <td>10359.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48126.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26225.000000</td>\n",
       "      <td>30706.000000</td>\n",
       "      <td>47219.000000</td>\n",
       "      <td>18341.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>2.540000</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>5.170000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48209.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30881.000000</td>\n",
       "      <td>35651.000000</td>\n",
       "      <td>57903.000000</td>\n",
       "      <td>22324.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>3.070000</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>6.440000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48221.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.400000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35168.000000</td>\n",
       "      <td>41259.000000</td>\n",
       "      <td>67968.000000</td>\n",
       "      <td>27902.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>3.630000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>7.730000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48228.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.580000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114458.000000</td>\n",
       "      <td>133902.000000</td>\n",
       "      <td>142467.000000</td>\n",
       "      <td>65681.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>9.720000</td>\n",
       "      <td>30.360000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48243.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Census Tract        GEOID  Median Income (Households)  \\\n",
       "count              447344       447344               447344.000000   \n",
       "unique                261          261                         NaN   \n",
       "top     Census Tract 5458  26163545800                         NaN   \n",
       "freq                 4721         4721                         NaN   \n",
       "mean                  NaN          NaN                32262.784300   \n",
       "std                   NaN          NaN                10739.229403   \n",
       "min                   NaN          NaN                11121.000000   \n",
       "25%                   NaN          NaN                26225.000000   \n",
       "50%                   NaN          NaN                30881.000000   \n",
       "75%                   NaN          NaN                35168.000000   \n",
       "max                   NaN          NaN               114458.000000   \n",
       "\n",
       "        Median Income (Families)  Median Income (Married Families)  \\\n",
       "count              447344.000000                     447344.000000   \n",
       "unique                       NaN                               NaN   \n",
       "top                          NaN                               NaN   \n",
       "freq                         NaN                               NaN   \n",
       "mean                38602.177946                      59586.092484   \n",
       "std                 13419.680392                      17912.912338   \n",
       "min                 14449.000000                      15182.000000   \n",
       "25%                 30706.000000                      47219.000000   \n",
       "50%                 35651.000000                      57903.000000   \n",
       "75%                 41259.000000                      67968.000000   \n",
       "max                133902.000000                     142467.000000   \n",
       "\n",
       "        Median Income (Nonfamily Households) Less than High School (%)  \\\n",
       "count                          447344.000000                    447344   \n",
       "unique                                   NaN                       213   \n",
       "top                                      NaN                       0.0   \n",
       "freq                                     NaN                     19231   \n",
       "mean                            23700.777858                       NaN   \n",
       "std                              8007.141849                       NaN   \n",
       "min                             10359.000000                       NaN   \n",
       "25%                             18341.000000                       NaN   \n",
       "50%                             22324.000000                       NaN   \n",
       "75%                             27902.000000                       NaN   \n",
       "max                             65681.000000                       NaN   \n",
       "\n",
       "       High School Graduate (%) Some College or Associate Degree (%)  \\\n",
       "count                    447344                               447344   \n",
       "unique                      215                                  207   \n",
       "top                        26.4                                  0.0   \n",
       "freq                       7098                                13336   \n",
       "mean                        NaN                                  NaN   \n",
       "std                         NaN                                  NaN   \n",
       "min                         NaN                                  NaN   \n",
       "25%                         NaN                                  NaN   \n",
       "50%                         NaN                                  NaN   \n",
       "75%                         NaN                                  NaN   \n",
       "max                         NaN                                  NaN   \n",
       "\n",
       "       Bachelor's Degree or Higher (%)  ...  Other Crime Rate  \\\n",
       "count                           447344  ...     447344.000000   \n",
       "unique                              78  ...               NaN   \n",
       "top                                0.0  ...               NaN   \n",
       "freq                            247788  ...               NaN   \n",
       "mean                               NaN  ...          0.896529   \n",
       "std                                NaN  ...          0.392914   \n",
       "min                                NaN  ...          0.290000   \n",
       "25%                                NaN  ...          0.650000   \n",
       "50%                                NaN  ...          0.840000   \n",
       "75%                                NaN  ...          1.050000   \n",
       "max                                NaN  ...          4.710000   \n",
       "\n",
       "        Property Crime Rate  Violent Crime Rate  Total Crime Rate     City  \\\n",
       "count         447344.000000       447344.000000     447344.000000   447344   \n",
       "unique                  NaN                 NaN               NaN        1   \n",
       "top                     NaN                 NaN               NaN  DETROIT   \n",
       "freq                    NaN                 NaN               NaN   447344   \n",
       "mean               3.253322            2.394017          6.684153      NaN   \n",
       "std                1.247353            0.946173          2.401048      NaN   \n",
       "min                1.160000            0.490000          1.980000      NaN   \n",
       "25%                2.540000            1.760000          5.170000      NaN   \n",
       "50%                3.070000            2.270000          6.440000      NaN   \n",
       "75%                3.630000            2.900000          7.730000      NaN   \n",
       "max               17.250000            9.720000         30.360000      NaN   \n",
       "\n",
       "            Address       Zip Code  \\\n",
       "count        441365  447344.000000   \n",
       "unique       211046            NaN   \n",
       "top     244 MADISON            NaN   \n",
       "freq             22            NaN   \n",
       "mean            NaN   48219.157557   \n",
       "std             NaN      10.821304   \n",
       "min             NaN   48126.000000   \n",
       "25%             NaN   48209.000000   \n",
       "50%             NaN   48221.000000   \n",
       "75%             NaN   48228.000000   \n",
       "max             NaN   48243.000000   \n",
       "\n",
       "                                          Location              Sale Date  \\\n",
       "count                                       447344                 447344   \n",
       "unique                                      212627                   5105   \n",
       "top     POINT (-83.0557768411244 42.3237930055647)  1/23/2014 12:00:00 AM   \n",
       "freq                                            48                   4628   \n",
       "mean                                           NaN                    NaN   \n",
       "std                                            NaN                    NaN   \n",
       "min                                            NaN                    NaN   \n",
       "25%                                            NaN                    NaN   \n",
       "50%                                            NaN                    NaN   \n",
       "75%                                            NaN                    NaN   \n",
       "max                                            NaN                    NaN   \n",
       "\n",
       "          Sale Price  \n",
       "count   4.473440e+05  \n",
       "unique           NaN  \n",
       "top              NaN  \n",
       "freq             NaN  \n",
       "mean    6.886125e+04  \n",
       "std     3.054315e+06  \n",
       "min     0.000000e+00  \n",
       "25%     1.000000e+00  \n",
       "50%     1.400000e+03  \n",
       "75%     2.580000e+04  \n",
       "max     7.000000e+08  \n",
       "\n",
       "[11 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for city, df in acs_commerce_crime_merged_with_sales.items():\n",
    "    print(f\"\\n📊 --- {city.upper()} ---\")\n",
    "    print(\"📌 DataFrame Info:\")\n",
    "    print(df.info())\n",
    "    print(\"\\n📈 Descriptive Statistics:\")\n",
    "    display(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f37f754",
   "metadata": {},
   "source": [
    "## Multi Linear Regression Test Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "414548c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Running regression for: Philadelphia\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             Sale Price   R-squared:                       0.012\n",
      "Model:                            OLS   Adj. R-squared:                  0.012\n",
      "Method:                 Least Squares   F-statistic:                     1743.\n",
      "Date:                Fri, 25 Apr 2025   Prob (F-statistic):               0.00\n",
      "Time:                        14:11:13   Log-Likelihood:            -4.5186e+07\n",
      "No. Observations:             2942097   AIC:                         9.037e+07\n",
      "Df Residuals:                 2942075   BIC:                         9.037e+07\n",
      "Df Model:                          21                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================================\n",
      "                                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "const                                   7.3e+07   1.69e+06     43.124      0.000    6.97e+07    7.63e+07\n",
      "Median Income (Households)              -4.6180      0.134    -34.568      0.000      -4.880      -4.356\n",
      "Median Income (Families)                 4.2398      0.087     48.626      0.000       4.069       4.411\n",
      "Median Income (Married Families)        -0.1060      0.069     -1.527      0.127      -0.242       0.030\n",
      "Median Income (Nonfamily Households)     3.6364      0.116     31.395      0.000       3.409       3.863\n",
      "Less than High School (%)            -3.952e+04   1.37e+04     -2.886      0.004   -6.64e+04   -1.27e+04\n",
      "High School Graduate (%)              -3.97e+04   1.37e+04     -2.899      0.004   -6.65e+04   -1.29e+04\n",
      "Some College or Associate Degree (%) -3.967e+04   1.37e+04     -2.897      0.004   -6.65e+04   -1.28e+04\n",
      "Bachelor's Degree or Higher (%)      -3.901e+04   1.37e+04     -2.849      0.004   -6.59e+04   -1.22e+04\n",
      "Total Population                        -7.5616      0.617    -12.252      0.000      -8.771      -6.352\n",
      "White (%)                            -1509.3244    116.075    -13.003      0.000   -1736.828   -1281.821\n",
      "Black (%)                            -1172.1564    115.437    -10.154      0.000   -1398.409    -945.903\n",
      "Asian (%)                             2308.4882    147.708     15.629      0.000    2018.986    2597.990\n",
      "Other (%)                            -1920.9810     83.751    -22.937      0.000   -2085.130   -1756.832\n",
      "Unemployment Rate (%)                 1964.8301    141.178     13.917      0.000    1688.126    2241.535\n",
      "Total Businesses                      6030.1827    137.114     43.979      0.000    5761.445    6298.920\n",
      "Nuisance Crime Rate                  -2.978e+05   1.06e+05     -2.819      0.005   -5.05e+05   -9.08e+04\n",
      "Other Crime Rate                     -2.975e+05   1.06e+05     -2.816      0.005   -5.05e+05   -9.04e+04\n",
      "Property Crime Rate                  -2.977e+05   1.06e+05     -2.818      0.005   -5.05e+05   -9.07e+04\n",
      "Violent Crime Rate                   -2.975e+05   1.06e+05     -2.816      0.005   -5.05e+05   -9.05e+04\n",
      "Total Crime Rate                      2.977e+05   1.06e+05      2.818      0.005    9.06e+04    5.05e+05\n",
      "Zip Code                             -3605.0487     53.086    -67.910      0.000   -3709.095   -3501.003\n",
      "==============================================================================\n",
      "Omnibus:                 11013391.881   Durbin-Watson:                   0.857\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):   12987822357369.654\n",
      "Skew:                          83.276   Prob(JB):                         0.00\n",
      "Kurtosis:                   10294.731   Cond. No.                     3.21e+08\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.21e+08. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "📊 Running regression for: Pittsburgh\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             Sale Price   R-squared:                       0.006\n",
      "Model:                            OLS   Adj. R-squared:                  0.006\n",
      "Method:                 Least Squares   F-statistic:                     198.5\n",
      "Date:                Fri, 25 Apr 2025   Prob (F-statistic):               0.00\n",
      "Time:                        14:11:15   Log-Likelihood:            -1.1193e+07\n",
      "No. Observations:              721128   AIC:                         2.239e+07\n",
      "Df Residuals:                  721104   BIC:                         2.239e+07\n",
      "Df Model:                          23                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================================\n",
      "                                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "const                                -1.517e+07   4.22e+06     -3.591      0.000   -2.34e+07   -6.89e+06\n",
      "Median Income (Households)              -1.8501      0.251     -7.380      0.000      -2.341      -1.359\n",
      "Median Income (Families)                 2.6753      0.248     10.796      0.000       2.190       3.161\n",
      "Median Income (Married Families)        -0.5378      0.210     -2.559      0.010      -0.950      -0.126\n",
      "Median Income (Nonfamily Households)    -0.2685      0.293     -0.917      0.359      -0.842       0.305\n",
      "Less than High School (%)             -1.36e+05   3.15e+04     -4.314      0.000   -1.98e+05   -7.42e+04\n",
      "High School Graduate (%)             -1.369e+05   3.15e+04     -4.343      0.000   -1.99e+05   -7.51e+04\n",
      "Some College or Associate Degree (%) -1.361e+05   3.15e+04     -4.316      0.000   -1.98e+05   -7.43e+04\n",
      "Bachelor's Degree or Higher (%)      -1.357e+05   3.15e+04     -4.307      0.000   -1.97e+05    -7.4e+04\n",
      "Total Population                         8.0394      2.270      3.542      0.000       3.591      12.488\n",
      "White (%)                             2.148e+04   1902.480     11.290      0.000    1.77e+04    2.52e+04\n",
      "Black (%)                             2.315e+04   1909.870     12.124      0.000    1.94e+04    2.69e+04\n",
      "Asian (%)                             2.163e+04   1935.427     11.177      0.000    1.78e+04    2.54e+04\n",
      "Other (%)                             8294.8508   1079.538      7.684      0.000    6178.992    1.04e+04\n",
      "Unemployment Rate (%)                 2836.7901    442.248      6.414      0.000    1969.999    3703.581\n",
      "Total Businesses                       184.6848      7.279     25.372      0.000     170.418     198.952\n",
      "INCIDENTTRACT                         3.622e+09   8.64e+11      0.004      0.997   -1.69e+12     1.7e+12\n",
      "Nuisance Crime Rate                  -1.925e+06   2.53e+05     -7.601      0.000   -2.42e+06   -1.43e+06\n",
      "Other Crime Rate                     -1.926e+06   2.53e+05     -7.609      0.000   -2.42e+06   -1.43e+06\n",
      "Property Crime Rate                  -1.928e+06   2.53e+05     -7.615      0.000   -2.42e+06   -1.43e+06\n",
      "Violent Crime Rate                   -1.929e+06   2.53e+05     -7.617      0.000   -2.43e+06   -1.43e+06\n",
      "Total Crime Rate                      1.927e+06   2.53e+05      7.610      0.000    1.43e+06    2.42e+06\n",
      "tract                                -3.622e+09   8.64e+11     -0.004      0.997    -1.7e+12    1.69e+12\n",
      "Zip Code                              1764.0957    193.784      9.103      0.000    1384.285    2143.907\n",
      "==============================================================================\n",
      "Omnibus:                  2184929.464   Durbin-Watson:                   1.666\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):     351307918848.989\n",
      "Skew:                          44.987   Prob(JB):                         0.00\n",
      "Kurtosis:                    3421.165   Cond. No.                     1.09e+14\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.19e-12. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "📊 Running regression for: Detroit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             Sale Price   R-squared:                       0.007\n",
      "Model:                            OLS   Adj. R-squared:                  0.007\n",
      "Method:                 Least Squares   F-statistic:                     125.1\n",
      "Date:                Fri, 25 Apr 2025   Prob (F-statistic):               0.00\n",
      "Time:                        14:11:16   Log-Likelihood:            -5.8662e+06\n",
      "No. Observations:              356370   AIC:                         1.173e+07\n",
      "Df Residuals:                  356348   BIC:                         1.173e+07\n",
      "Df Model:                          21                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================================\n",
      "                                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "const                                -2.632e+07   3.03e+07     -0.868      0.385   -8.57e+07    3.31e+07\n",
      "Median Income (Households)             -47.0316      1.988    -23.659      0.000     -50.928     -43.135\n",
      "Median Income (Families)                29.8877      1.280     23.350      0.000      27.379      32.396\n",
      "Median Income (Married Families)        -0.4731      0.547     -0.865      0.387      -1.545       0.599\n",
      "Median Income (Nonfamily Households)    35.7986      1.614     22.185      0.000      32.636      38.961\n",
      "Less than High School (%)             1164.3147    202.298      5.755      0.000     767.817    1560.812\n",
      "High School Graduate (%)               698.7323    220.167      3.174      0.002     267.212    1130.253\n",
      "Some College or Associate Degree (%) -1284.2774    202.014     -6.357      0.000   -1680.219    -888.336\n",
      "Bachelor's Degree or Higher (%)        419.9574    717.407      0.585      0.558    -986.139    1826.054\n",
      "Total Population                        -0.0890      6.837     -0.013      0.990     -13.488      13.310\n",
      "White (%)                              802.9368   1332.920      0.602      0.547   -1809.547    3415.420\n",
      "Black (%)                               14.3490    891.148      0.016      0.987   -1732.276    1760.974\n",
      "Asian (%)                            -1265.2413   1290.372     -0.981      0.327   -3794.333    1263.851\n",
      "Other (%)                             -311.5024     73.264     -4.252      0.000    -455.097    -167.908\n",
      "Unemployment Rate (%)                 5799.4693    858.109      6.758      0.000    4117.601    7481.337\n",
      "Total Businesses                      1.934e+04    742.933     26.027      0.000    1.79e+04    2.08e+04\n",
      "Nuisance Crime Rate                  -4.592e+06   9.16e+05     -5.016      0.000   -6.39e+06    -2.8e+06\n",
      "Other Crime Rate                     -5.471e+06   9.13e+05     -5.990      0.000   -7.26e+06   -3.68e+06\n",
      "Property Crime Rate                  -5.376e+06   9.14e+05     -5.879      0.000   -7.17e+06   -3.58e+06\n",
      "Violent Crime Rate                   -5.404e+06   9.13e+05     -5.918      0.000   -7.19e+06   -3.61e+06\n",
      "Total Crime Rate                      5.423e+06   9.14e+05      5.935      0.000    3.63e+06    7.21e+06\n",
      "Zip Code                               524.7106    628.955      0.834      0.404    -708.024    1757.445\n",
      "==============================================================================\n",
      "Omnibus:                  1722293.200   Durbin-Watson:                   0.295\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):   22666978784805.414\n",
      "Skew:                         192.155   Prob(JB):                         0.00\n",
      "Kurtosis:                   39071.871   Cond. No.                     5.15e+08\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.15e+08. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def run_regression_for_all_cities(merged_data_dict):\n",
    "    results = {}\n",
    "\n",
    "    for city, df in merged_data_dict.items():\n",
    "        print(f\"\\n📊 Running regression for: {city.title()}\")\n",
    "\n",
    "        # Drop missing or zero Sale Prices\n",
    "        df = df[df[\"Sale Price\"].notna() & (df[\"Sale Price\"] > 0)].copy()\n",
    "\n",
    "        # Convert education columns to numeric\n",
    "        edu_cols = [\n",
    "            'Less than High School (%)',\n",
    "            'High School Graduate (%)',\n",
    "            'Some College or Associate Degree (%)',\n",
    "            \"Bachelor's Degree or Higher (%)\"\n",
    "        ]\n",
    "        for col in edu_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # Drop rows with NaNs in any numeric column\n",
    "        numeric_df = df.select_dtypes(include=['float64', 'int64']).dropna()\n",
    "\n",
    "        # Separate X and y\n",
    "        y = numeric_df['Sale Price']\n",
    "        X = numeric_df.drop(columns=['Sale Price'])\n",
    "\n",
    "        # Add constant for intercept\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # Fit model\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        results[city] = model\n",
    "\n",
    "        # Print summary\n",
    "        print(model.summary())\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the function\n",
    "regression_models = run_regression_for_all_cities(acs_commerce_crime_merged_with_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a225136",
   "metadata": {},
   "source": [
    "## Log Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e1d50b01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Running Log-Linear Regression for: Philadelphia\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             Sale Price   R-squared:                       0.182\n",
      "Model:                            OLS   Adj. R-squared:                  0.182\n",
      "Method:                 Least Squares   F-statistic:                 3.120e+04\n",
      "Date:                Fri, 25 Apr 2025   Prob (F-statistic):               0.00\n",
      "Time:                        14:11:24   Log-Likelihood:            -3.8241e+06\n",
      "No. Observations:             2942097   AIC:                         7.648e+06\n",
      "Df Residuals:                 2942075   BIC:                         7.649e+06\n",
      "Df Model:                          21                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================================\n",
      "                                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "const                                  182.3770      1.328    137.372      0.000     179.775     184.979\n",
      "Median Income (Households)           -3.887e-06   1.05e-07    -37.104      0.000   -4.09e-06   -3.68e-06\n",
      "Median Income (Families)              3.988e-06   6.84e-08     58.313      0.000    3.85e-06    4.12e-06\n",
      "Median Income (Married Families)      2.194e-06   5.44e-08     40.303      0.000    2.09e-06     2.3e-06\n",
      "Median Income (Nonfamily Households)  9.802e-06   9.08e-08    107.908      0.000    9.62e-06    9.98e-06\n",
      "Less than High School (%)                0.0255      0.011      2.376      0.017       0.004       0.047\n",
      "High School Graduate (%)                 0.0254      0.011      2.368      0.018       0.004       0.046\n",
      "Some College or Associate Degree (%)     0.0248      0.011      2.305      0.021       0.004       0.046\n",
      "Bachelor's Degree or Higher (%)          0.0262      0.011      2.440      0.015       0.005       0.047\n",
      "Total Population                      9.086e-07   4.84e-07      1.877      0.061   -4.01e-08    1.86e-06\n",
      "White (%)                                0.0067    9.1e-05     73.205      0.000       0.006       0.007\n",
      "Black (%)                                0.0035   9.05e-05     39.013      0.000       0.003       0.004\n",
      "Asian (%)                                0.0137      0.000    118.608      0.000       0.014       0.014\n",
      "Other (%)                               -0.0007   6.57e-05    -10.493      0.000      -0.001      -0.001\n",
      "Unemployment Rate (%)                    0.0004      0.000      3.283      0.001       0.000       0.001\n",
      "Total Businesses                         0.0024      0.000     22.301      0.000       0.002       0.003\n",
      "Nuisance Crime Rate                      3.2710      0.083     39.484      0.000       3.109       3.433\n",
      "Other Crime Rate                         3.2726      0.083     39.503      0.000       3.110       3.435\n",
      "Property Crime Rate                      3.2714      0.083     39.489      0.000       3.109       3.434\n",
      "Violent Crime Rate                       3.2709      0.083     39.481      0.000       3.108       3.433\n",
      "Total Crime Rate                        -3.2716      0.083    -39.491      0.000      -3.434      -3.109\n",
      "Zip Code                                -0.0091   4.16e-05   -219.347      0.000      -0.009      -0.009\n",
      "==============================================================================\n",
      "Omnibus:                   385087.314   Durbin-Watson:                   1.166\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1175064.220\n",
      "Skew:                           0.692   Prob(JB):                         0.00\n",
      "Kurtosis:                       5.769   Cond. No.                     3.21e+08\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.21e+08. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "📊 Running Log-Linear Regression for: Pittsburgh\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             Sale Price   R-squared:                       0.102\n",
      "Model:                            OLS   Adj. R-squared:                  0.102\n",
      "Method:                 Least Squares   F-statistic:                     3548.\n",
      "Date:                Fri, 25 Apr 2025   Prob (F-statistic):               0.00\n",
      "Time:                        14:11:26   Log-Likelihood:            -1.0628e+06\n",
      "No. Observations:              721128   AIC:                         2.126e+06\n",
      "Df Residuals:                  721104   BIC:                         2.126e+06\n",
      "Df Model:                          23                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================================\n",
      "                                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "const                                  -45.5181      3.348    -13.594      0.000     -52.081     -38.955\n",
      "Median Income (Households)           -1.494e-06   1.99e-07     -7.516      0.000   -1.88e-06    -1.1e-06\n",
      "Median Income (Families)              5.441e-06   1.96e-07     27.691      0.000    5.06e-06    5.83e-06\n",
      "Median Income (Married Families)       9.16e-07   1.67e-07      5.498      0.000    5.89e-07    1.24e-06\n",
      "Median Income (Nonfamily Households) -7.459e-06   2.32e-07    -32.125      0.000   -7.91e-06      -7e-06\n",
      "Less than High School (%)               -0.3704      0.025    -14.818      0.000      -0.419      -0.321\n",
      "High School Graduate (%)                -0.3721      0.025    -14.883      0.000      -0.421      -0.323\n",
      "Some College or Associate Degree (%)    -0.3728      0.025    -14.906      0.000      -0.422      -0.324\n",
      "Bachelor's Degree or Higher (%)         -0.3688      0.025    -14.758      0.000      -0.418      -0.320\n",
      "Total Population                      7.429e-05    1.8e-06     41.278      0.000    7.08e-05    7.78e-05\n",
      "White (%)                                0.0400      0.002     26.518      0.000       0.037       0.043\n",
      "Black (%)                                0.0424      0.002     27.998      0.000       0.039       0.045\n",
      "Asian (%)                                0.0373      0.002     24.282      0.000       0.034       0.040\n",
      "Other (%)                                0.0186      0.001     21.751      0.000       0.017       0.020\n",
      "Unemployment Rate (%)                    0.0091      0.000     25.991      0.000       0.008       0.010\n",
      "Total Businesses                      8.214e-05   5.77e-06     14.231      0.000    7.08e-05    9.34e-05\n",
      "INCIDENTTRACT                        -4.148e+05   6.85e+05     -0.605      0.545   -1.76e+06    9.28e+05\n",
      "Nuisance Crime Rate                     -7.7154      0.201    -38.428      0.000      -8.109      -7.322\n",
      "Other Crime Rate                        -7.7271      0.201    -38.496      0.000      -8.120      -7.334\n",
      "Property Crime Rate                     -7.7222      0.201    -38.469      0.000      -8.116      -7.329\n",
      "Violent Crime Rate                      -7.7390      0.201    -38.543      0.000      -8.132      -7.345\n",
      "Total Crime Rate                         7.7260      0.201     38.488      0.000       7.333       8.119\n",
      "tract                                 4.148e+05   6.85e+05      0.605      0.545   -9.28e+05    1.76e+06\n",
      "Zip Code                                 0.0060      0.000     38.732      0.000       0.006       0.006\n",
      "==============================================================================\n",
      "Omnibus:                    22292.619   Durbin-Watson:                   1.566\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            54730.339\n",
      "Skew:                           0.130   Prob(JB):                         0.00\n",
      "Kurtosis:                       4.324   Cond. No.                     1.09e+14\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.19e-12. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "📊 Running Log-Linear Regression for: Detroit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             Sale Price   R-squared:                       0.039\n",
      "Model:                            OLS   Adj. R-squared:                  0.039\n",
      "Method:                 Least Squares   F-statistic:                     687.1\n",
      "Date:                Fri, 25 Apr 2025   Prob (F-statistic):               0.00\n",
      "Time:                        14:11:27   Log-Likelihood:            -9.9286e+05\n",
      "No. Observations:              356370   AIC:                         1.986e+06\n",
      "Df Residuals:                  356348   BIC:                         1.986e+06\n",
      "Df Model:                          21                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================================\n",
      "                                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "const                                 -252.5544     34.883     -7.240      0.000    -320.923    -184.185\n",
      "Median Income (Households)            2.959e-06   2.29e-06      1.293      0.196   -1.53e-06    7.44e-06\n",
      "Median Income (Families)               2.96e-06   1.47e-06      2.009      0.045    7.24e-08    5.85e-06\n",
      "Median Income (Married Families)      8.426e-06    6.3e-07     13.383      0.000    7.19e-06    9.66e-06\n",
      "Median Income (Nonfamily Households)   2.81e-05   1.86e-06     15.130      0.000    2.45e-05    3.17e-05\n",
      "Less than High School (%)               -0.0029      0.000    -12.379      0.000      -0.003      -0.002\n",
      "High School Graduate (%)                 0.0003      0.000      1.071      0.284      -0.000       0.001\n",
      "Some College or Associate Degree (%)     0.0002      0.000      0.940      0.347      -0.000       0.001\n",
      "Bachelor's Degree or Higher (%)          0.0012      0.001      1.433      0.152      -0.000       0.003\n",
      "Total Population                       5.17e-05   7.87e-06      6.570      0.000    3.63e-05    6.71e-05\n",
      "White (%)                                0.0087      0.002      5.649      0.000       0.006       0.012\n",
      "Black (%)                                0.0041      0.001      3.981      0.000       0.002       0.006\n",
      "Asian (%)                                0.0071      0.001      4.796      0.000       0.004       0.010\n",
      "Other (%)                               -0.0018   8.43e-05    -21.484      0.000      -0.002      -0.002\n",
      "Unemployment Rate (%)                   -0.0133      0.001    -13.423      0.000      -0.015      -0.011\n",
      "Total Businesses                         0.0037      0.001      4.330      0.000       0.002       0.005\n",
      "Nuisance Crime Rate                     -0.4942      1.054     -0.469      0.639      -2.560       1.571\n",
      "Other Crime Rate                         0.7976      1.051      0.759      0.448      -1.263       2.858\n",
      "Property Crime Rate                      1.1011      1.053      1.046      0.296      -0.962       3.164\n",
      "Violent Crime Rate                       0.3295      1.051      0.314      0.754      -1.730       2.389\n",
      "Total Crime Rate                        -0.6590      1.052     -0.627      0.531      -2.720       1.402\n",
      "Zip Code                                 0.0053      0.001      7.388      0.000       0.004       0.007\n",
      "==============================================================================\n",
      "Omnibus:                    35962.991   Durbin-Watson:                   1.524\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            46691.813\n",
      "Skew:                          -0.875   Prob(JB):                         0.00\n",
      "Kurtosis:                       2.719   Cond. No.                     5.15e+08\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.15e+08. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# === PREPARE DATA ===\n",
    "def preprocess_for_modeling(df):\n",
    "    df = df.copy()\n",
    "    df = df[df[\"Sale Price\"].notna() & (df[\"Sale Price\"] > 0)]\n",
    "\n",
    "    # Convert education columns to numeric if not already\n",
    "    education_cols = [\n",
    "        'Less than High School (%)',\n",
    "        'High School Graduate (%)',\n",
    "        'Some College or Associate Degree (%)',\n",
    "        \"Bachelor's Degree or Higher (%)\"\n",
    "    ]\n",
    "    for col in education_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).drop(columns=['Sale Price']).columns\n",
    "    df = df.dropna(subset=numeric_cols)\n",
    "\n",
    "    X = df[numeric_cols]\n",
    "    y = df[\"Sale Price\"]\n",
    "    return X, y\n",
    "\n",
    "# === LOG-LINEAR REGRESSION ===\n",
    "def log_linear_regression(X, y):\n",
    "    y_log = np.log(y)\n",
    "    X_const = sm.add_constant(X)\n",
    "    model = sm.OLS(y_log, X_const).fit()\n",
    "    return model\n",
    "\n",
    "# === RUN FOR ALL CITIES ===\n",
    "def run_log_linear_for_all_cities(data_dict):\n",
    "    results = {}\n",
    "    for city, df in data_dict.items():\n",
    "        print(f\"\\n📊 Running Log-Linear Regression for: {city.title()}\")\n",
    "        X, y = preprocess_for_modeling(df)\n",
    "        model = log_linear_regression(X, y)\n",
    "        print(model.summary())\n",
    "        results[city] = model\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "log_linear_models = run_log_linear_for_all_cities(acs_commerce_crime_merged_with_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62aa557",
   "metadata": {},
   "source": [
    "## Ridge Lasso Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3084222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Running Ridge & Lasso Regression for: Philadelphia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junsik/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.334e+18, tolerance: 2.981e+14\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge R^2: 0.0122 | Lasso R^2: 0.0122\n",
      "\n",
      "📈 Running Ridge & Lasso Regression for: Pittsburgh\n"
     ]
    }
   ],
   "source": [
    "# # === RIDGE AND LASSO REGRESSION ===\n",
    "# def ridge_lasso_regression(X, y):\n",
    "#     X_scaled = StandardScaler().fit_transform(X)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     ridge = Ridge(alpha=1.0).fit(X_train, y_train)\n",
    "#     lasso = Lasso(alpha=0.1).fit(X_train, y_train)\n",
    "\n",
    "#     return {\n",
    "#         \"ridge_r2\": ridge.score(X_test, y_test),\n",
    "#         \"lasso_r2\": lasso.score(X_test, y_test),\n",
    "#         \"ridge_model\": ridge,\n",
    "#         \"lasso_model\": lasso\n",
    "#     }\n",
    "\n",
    "# # === RUN RIDGE & LASSO FOR ALL CITIES ===\n",
    "# def run_ridge_lasso_for_all_cities(data_dict):\n",
    "#     results = {}\n",
    "#     for city, df in data_dict.items():\n",
    "#         print(f\"\\n📈 Running Ridge & Lasso Regression for: {city.title()}\")\n",
    "#         X, y = preprocess_for_modeling(df)\n",
    "#         res = ridge_lasso_regression(X, y)\n",
    "#         print(f\"Ridge R^2: {res['ridge_r2']:.4f} | Lasso R^2: {res['lasso_r2']:.4f}\")\n",
    "#         results[city] = res\n",
    "#     return results\n",
    "\n",
    "# ridge_lasso_models = run_ridge_lasso_for_all_cities(acs_commerce_crime_merged_with_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b6ef3",
   "metadata": {},
   "source": [
    "## Quantile Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ac26f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# === QUANTILE REGRESSION ===\n",
    "def quantile_regression(X, y, quantile=0.5):\n",
    "    X_const = sm.add_constant(X)\n",
    "    model = sm.QuantReg(np.log(y), X_const).fit(q=quantile)\n",
    "    return model\n",
    "\n",
    "# === RUN QUANTILE FOR ALL CITIES ===\n",
    "def run_quantile_for_all_cities(data_dict, quantiles=[0.25, 0.5, 0.75]):\n",
    "    results = {}\n",
    "    for city, df in data_dict.items():\n",
    "        print(f\"\\n📊 Running Quantile Regressions for: {city.title()}\")\n",
    "        X, y = preprocess_for_modeling(df)\n",
    "        city_results = {}\n",
    "        for q in quantiles:\n",
    "            print(f\"\\nQuantile: {q}\")\n",
    "            model = quantile_regression(X, y, quantile=q)\n",
    "            print(model.summary())\n",
    "            city_results[q] = model\n",
    "        results[city] = city_results\n",
    "    return results\n",
    "\n",
    "quantile_models = run_quantile_for_all_cities(acs_commerce_crime_merged_with_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4fe59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a76672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# R-squared values from your regressions\n",
    "r2_data = {\n",
    "    'Philadelphia': {\n",
    "        'Multi Linear': 0.012,\n",
    "        'Log-Linear': 0.182,\n",
    "        'Ridge': 0.0122,\n",
    "        'Lasso': 0.0122,\n",
    "        'Quantile (0.25)': 0.1036,\n",
    "        'Quantile (0.5)': 0.1215,\n",
    "        'Quantile (0.75)': 0.1333\n",
    "    },\n",
    "    'Pittsburgh': {\n",
    "        'Multi Linear': 0.006,\n",
    "        'Log-Linear': 0.102,\n",
    "        'Ridge': 0.0068,\n",
    "        'Lasso': 0.0068,\n",
    "        'Quantile (0.25)': 0.06414,\n",
    "        'Quantile (0.5)': 0.06334,\n",
    "        'Quantile (0.75)': 0.07584\n",
    "    },\n",
    "    'Detroit': {\n",
    "        'Multi Linear': 0.007,\n",
    "        'Log-Linear': 0.039,\n",
    "        'Ridge': 0.0045,\n",
    "        'Lasso': 0.0044,\n",
    "        'Quantile (0.25)': 0.01850,\n",
    "        'Quantile (0.5)': 0.05188,\n",
    "        'Quantile (0.75)': 0.04721\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_r2 = pd.DataFrame(r2_data).reset_index().rename(columns={'index': 'Model'})\n",
    "df_r2 = pd.melt(df_r2, id_vars='Model', var_name='City', value_name='R_squared')\n",
    "\n",
    "# Create bar chart\n",
    "fig = px.bar(df_r2, x='City', y='R_squared', color='Model', barmode='group',\n",
    "             title='R² Values Across Regression Models by City',\n",
    "             labels={'R_squared': 'R² Value'},\n",
    "             height=600)\n",
    "\n",
    "# Transparent background\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    paper_bgcolor='rgba(0,0,0,0)',\n",
    "    yaxis=dict(range=[0, 0.2]),\n",
    "    legend_title_text='Model',\n",
    "    font=dict(size=14)\n",
    ")\n",
    "\n",
    "# Save figure (install kaleido first!)\n",
    "# \n",
    "fig.write_image(\"regression_r2_comparison_plotly.png\", format=\"png\")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076bfcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Top features from log-linear models\n",
    "top_features = {\n",
    "    \"Philadelphia\": [\n",
    "        (\"Asian (%)\", 0.0137, \"<0.001\"),\n",
    "        (\"Bachelor's Degree or Higher (%)\", 0.0262, \"0.015\"),\n",
    "        (\"Median Income (Nonfamily Households)\", 9.802e-06, \"<0.001\"),\n",
    "    ],\n",
    "    \"Pittsburgh\": [\n",
    "        (\"Black (%)\", 0.0424, \"<0.001\"),\n",
    "        (\"Asian (%)\", 0.0373, \"<0.001\"),\n",
    "        (\"White (%)\", 0.0400, \"<0.001\"),\n",
    "    ],\n",
    "    \"Detroit\": [\n",
    "        (\"Asian (%)\", 0.0071, \"<0.001\"),\n",
    "        (\"White (%)\", 0.0087, \"<0.001\"),\n",
    "        (\"Total Population\", 5.17e-05, \"<0.001\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Format for Plotly Table\n",
    "rows = []\n",
    "colors = []\n",
    "city_colors = {\n",
    "    \"Philadelphia\": '#1e88e5',  # Medium blue\n",
    "    \"Pittsburgh\": '#90caf9',    # Light blue\n",
    "    \"Detroit\": '#ffffff'        # White\n",
    "}\n",
    "\n",
    "for city, features in top_features.items():\n",
    "    for feature, coef, pval in features:\n",
    "        rows.append({\n",
    "            \"City\": city,\n",
    "            \"Feature\": feature,\n",
    "            \"Coefficient\": coef,\n",
    "            \"p-value\": pval\n",
    "        })\n",
    "        colors.append(city_colors[city])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Create interactive color-coded table\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=[\"<b>City</b>\", \"<b>Feature</b>\", \"<b>Coefficient</b>\", \"<b>p-value</b>\"],\n",
    "        fill_color='royalblue',\n",
    "        font=dict(color='white', size=14),\n",
    "        align='left'\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[df.City, df.Feature, df.Coefficient.round(5), df['p-value']],\n",
    "        fill_color=[colors],\n",
    "        align='left',\n",
    "        font=dict(size=13, color='black')\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Top Predictors of Sale Price (Log-Linear Regression)\",\n",
    "    title_font_size=20,\n",
    "    margin=dict(l=20, r=20, t=60, b=20),\n",
    "    paper_bgcolor='rgba(0,0,0,0)'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c8514",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use a clean and appealing theme\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# Custom color palette for city comparison\n",
    "city_palette = {\n",
    "    \"Philadelphia\": \"#1f77b4\",  # Soft blue\n",
    "    \"Pittsburgh\": \"#2ca02c\",    # Medium green\n",
    "    \"Detroit\": \"#ff7f0e\"        # Orange\n",
    "}\n",
    "\n",
    "# === BAR CHART: Top LightGBM Feature Importances by City ===\n",
    "top_n = 5\n",
    "importance_df = pd.DataFrame()\n",
    "\n",
    "for city, metrics in results.items():\n",
    "    fi = pd.DataFrame({\n",
    "        'Feature': metrics['X_train'].columns,\n",
    "        'Importance': metrics['lgb_model'].feature_importances_,\n",
    "        'City': city.title()  # Capitalize to match palette keys\n",
    "    })\n",
    "    importance_df = pd.concat([importance_df, fi.nlargest(top_n, 'Importance')])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=importance_df, x='Importance', y='Feature', hue='City', palette=city_palette)\n",
    "plt.title(\"Top LightGBM Feature Importances by City\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.legend(title=\"City\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top_importances_by_city.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a16de6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# === CORRELATION HEATMAPS FOR EACH CITY ===\n",
    "for city in results:\n",
    "    X_corr = results[city]['X_train'].corr()\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(X_corr, cmap=\"crest\", center=0, annot=False, cbar_kws={'shrink': 0.7})\n",
    "    plt.title(f\"{city.title()} - Feature Correlation Matrix\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{city.lower()}_correlation_heatmap.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9bece",
   "metadata": {},
   "source": [
    "## Machine Learning Algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# === PREPROCESSING FUNCTION ===\n",
    "def preprocess_for_modeling(df):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame for modeling:\n",
    "    - Remove rows with missing or zero 'Sale Price'.\n",
    "    - Convert education columns (given in percentage as strings) to numeric.\n",
    "    - Drop rows with NaNs in the numeric columns (excluding 'Sale Price').\n",
    "    Returns predictors X and target y.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df[df[\"Sale Price\"].notna() & (df[\"Sale Price\"] > 0)]\n",
    "    \n",
    "    # Convert education columns to numeric if not already numeric\n",
    "    education_cols = [\n",
    "        'Less than High School (%)',\n",
    "        'High School Graduate (%)',\n",
    "        'Some College or Associate Degree (%)',\n",
    "        \"Bachelor's Degree or Higher (%)\"\n",
    "    ]\n",
    "    for col in education_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Select only numeric columns (excluding the target)\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).drop(columns=['Sale Price'], errors='ignore').columns\n",
    "    df = df.dropna(subset=numeric_cols)\n",
    "    \n",
    "    X = df[numeric_cols]\n",
    "    y = df[\"Sale Price\"]\n",
    "    return X, y\n",
    "\n",
    "# === LOAD YOUR DATA ===\n",
    "# For this example, we'll use the Philadelphia dataset.\n",
    "# In your context, 'acs_commerce_crime_merged_with_sales' is already defined.\n",
    "final_data = acs_commerce_crime_merged_with_sales['philadelphia']\n",
    "\n",
    "# Preprocess the data\n",
    "X, y = preprocess_for_modeling(final_data)\n",
    "print(f\"Preprocessed data: X shape = {X.shape}, y shape = {y.shape}\")\n",
    "\n",
    "# === TRAIN-TEST SPLIT ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de2eaa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === PREPROCESSING FUNCTION ===\n",
    "def preprocess_for_modeling(df):\n",
    "    df = df.copy()\n",
    "    # Keep only rows with valid and positive Sale Price\n",
    "    df = df[df[\"Sale Price\"].notna() & (df[\"Sale Price\"] > 0)]\n",
    "    \n",
    "    # Convert education columns to numeric if not already\n",
    "    education_cols = [\n",
    "        'Less than High School (%)',\n",
    "        'High School Graduate (%)',\n",
    "        'Some College or Associate Degree (%)',\n",
    "        \"Bachelor's Degree or Higher (%)\"\n",
    "    ]\n",
    "    for col in education_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Select only numeric columns (excluding the target)\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).drop(columns=['Sale Price'], errors='ignore').columns\n",
    "    df = df.dropna(subset=numeric_cols)\n",
    "    \n",
    "    X = df[numeric_cols]\n",
    "    y = df[\"Sale Price\"]\n",
    "    return X, y\n",
    "\n",
    "# === INITIALIZE RESULTS DICTIONARIES ===\n",
    "results = {}\n",
    "cities = list(acs_commerce_crime_merged_with_sales.keys())\n",
    "\n",
    "for city in cities:\n",
    "    print(f\"\\n===== Processing {city.title()} Dataset =====\")\n",
    "    \n",
    "    # Load data for the current city\n",
    "    data = acs_commerce_crime_merged_with_sales[city]\n",
    "    \n",
    "    # Preprocess\n",
    "    X, y = preprocess_for_modeling(data)\n",
    "    print(f\"Preprocessed data for {city.title()}: X shape = {X.shape}, y shape = {y.shape}\")\n",
    "    \n",
    "    # Train-test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # === XGBOOST REGRESSION ===\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "    r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "    print(f\"{city.title()} - XGBoost RMSE: {rmse_xgb:.2f}, R²: {r2_xgb:.3f}\")\n",
    "    \n",
    "    # === LIGHTGBM REGRESSION ===\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    y_pred_lgb = lgb_model.predict(X_test)\n",
    "    rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\n",
    "    r2_lgb = r2_score(y_test, y_pred_lgb)\n",
    "    print(f\"{city.title()} - LightGBM RMSE: {rmse_lgb:.2f}, R²: {r2_lgb:.3f}\")\n",
    "    \n",
    "    # Store models and metrics\n",
    "    results[city] = {\n",
    "        'rmse_xgb': rmse_xgb,\n",
    "        'r2_xgb': r2_xgb,\n",
    "        'rmse_lgb': rmse_lgb,\n",
    "        'r2_lgb': r2_lgb,\n",
    "        'xgb_model': xgb_model,\n",
    "        'lgb_model': lgb_model,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_pred_xgb': y_pred_xgb,\n",
    "        'y_pred_lgb': y_pred_lgb\n",
    "    }\n",
    "    \n",
    "# === VISUALIZATIONS: LightGBM Feature Importances for each city ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for city, metrics in results.items():\n",
    "    # Extract the feature names and LightGBM importances\n",
    "    features = metrics['X_train'].columns\n",
    "    lgb_importances = metrics['lgb_model'].feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for LightGBM importances\n",
    "    fi_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'LightGBM Importance': lgb_importances\n",
    "    })\n",
    "    # Sort descending by importance\n",
    "    fi_df.sort_values(by='LightGBM Importance', ascending=False, inplace=True)\n",
    "    \n",
    "    # Plot only LightGBM's feature importances\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='LightGBM Importance', y='Feature', data=fi_df, palette='viridis')\n",
    "    plt.title(f\"{city.title()} - LightGBM Feature Importances\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Save the figure\n",
    "    filename = f\"{output_dir}/{city.lower().replace(' ', '_')}_lightgbm_importance.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight', transparent=True)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RANDOM FOREST REGRESSION ===\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100, \n",
    "    max_depth=None, \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest RMSE: {rmse_rf:.2f}, R²: {r2_rf:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92796fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VISUALIZATIONS ===\n",
    "\n",
    "# 1. FEATURE IMPORTANCE COMPARISON\n",
    "feature_names = X_train.columns\n",
    "gb_importances = gb_model.feature_importances_\n",
    "rf_importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for side-by-side feature importance comparison.\n",
    "fi_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'GB Importance': gb_importances,\n",
    "    'RF Importance': rf_importances\n",
    "}).sort_values(by='GB Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='GB Importance', y='Feature', data=fi_df, color='green', label='Gradient Boosting')\n",
    "sns.barplot(x='RF Importance', y='Feature', data=fi_df, color='blue', alpha=0.6, label='Random Forest')\n",
    "plt.title(\"Feature Importances: Gradient Boosting vs. Random Forest\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8528b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ACTUAL VS. PREDICTED SCATTER PLOT (GRADIENT BOOSTING)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_gb, alpha=0.3, color='green')\n",
    "plt.xlabel(\"Actual Sale Price\")\n",
    "plt.ylabel(\"Predicted Sale Price (GB)\")\n",
    "plt.title(\"Gradient Boosting: Actual vs. Predicted Sale Price\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', linewidth=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 3. RESIDUAL DISTRIBUTION (RANDOM FOREST)\n",
    "rf_residuals = y_test - y_pred_rf\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(rf_residuals, bins=50, kde=True, color='blue')\n",
    "plt.xlabel(\"Residuals (RF Actual - Predicted)\")\n",
    "plt.title(\"Random Forest Residual Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
